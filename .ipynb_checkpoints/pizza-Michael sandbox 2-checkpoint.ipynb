{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Shuffle data\n",
    "*transform and not fit_transform\n",
    "*Where did train_target come from? in In [11]. I changed code to provide explicit reference at input\n",
    "*What's the idea behind makign train_data 3x the size of dev_data? I know train is usually bigger but just wondering if you have a specific idea   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = 'train.json'\n",
    "test_dataset = 'test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def separate_data(dataset,data_type):\n",
    "    data_features=[]\n",
    "    data_msgtxt=[]\n",
    "    data_titletxt=[]\n",
    "    data_subreddits=[]\n",
    "    data_ts=[]\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        data_target=[]\n",
    "    \n",
    "    '''\n",
    "    for element in predict_dataset[0]:\n",
    "        data_elements.append(element)\n",
    "    '''\n",
    "    \n",
    "    for request in dataset:\n",
    "        parts = []\n",
    "        \n",
    "        data_msgtxt.append(request['request_text_edit_aware'])\n",
    "        data_titletxt.append(request['request_title'])\n",
    "        data_subreddits.append(request['requester_subreddits_at_request'])\n",
    "        \n",
    "        month = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%m\")\n",
    "        day_of_month = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%d\")\n",
    "        hour = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%H\")\n",
    "        #rqst_ts = [int(month),int(day_of_month),int(hour)]\n",
    "        rqst_ts = [int(hour)]\n",
    "        \n",
    "        if data_type == 'train':\n",
    "            if request['requester_received_pizza'] == True:\n",
    "                data_target.append(1)\n",
    "            else:\n",
    "                data_target.append(0)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            parts.append(request[element])\n",
    "        \n",
    "        data_features.append(parts)\n",
    "        data_ts.append(rqst_ts)\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        return data_target, data_msgtxt, data_titletxt, data_subreddits, data_features, data_ts\n",
    "    else:\n",
    "        return data_msgtxt, data_titletxt, data_subreddits, data_features, data_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n",
      "3030\n",
      "1631\n",
      "[u'requester_days_since_first_post_on_raop_at_request',\n",
      " u'requester_account_age_in_days_at_request',\n",
      " u'requester_number_of_posts_on_raop_at_request',\n",
      " u'requester_number_of_posts_at_request',\n",
      " u'requester_upvotes_plus_downvotes_at_request',\n",
      " u'requester_number_of_comments_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_request',\n",
      " u'requester_number_of_comments_in_raop_at_request',\n",
      " u'requester_number_of_subreddits_at_request']\n",
      "[0.0, 103.10547453703704, 0, 2, 24, 3, 18, 0, 3]\n",
      "[9]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Load data from JSON\n",
    "train_data = json.loads(open(train_dataset).read())\n",
    "test_data = json.loads(open(test_dataset).read())\n",
    "\n",
    "#Row titles\n",
    "data_elements=[u'requester_days_since_first_post_on_raop_at_request', u'requester_account_age_in_days_at_request', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_at_request', u'requester_upvotes_plus_downvotes_at_request', u'requester_number_of_comments_at_request', u'requester_upvotes_minus_downvotes_at_request',u'requester_number_of_comments_in_raop_at_request',u'requester_number_of_subreddits_at_request']\n",
    "\n",
    "# Set np seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#Shuffle and split\n",
    "np.random.shuffle(train_data) #list is mutable, this shuffles permanently\n",
    "num_train = len(train_data)\n",
    "train_raw_data=train_data[num_train/4:] #train is 3x bigger than dev\n",
    "dev_raw_data=train_data[:num_train/4]\n",
    "test_raw_data = test_data\n",
    "\n",
    "train_target, train_msgtxt, train_titletxt, train_subreddits, train_features, train_ts = separate_data(train_raw_data,'train')\n",
    "dev_target, dev_msgtxt, dev_titletxt, dev_subreddits, dev_features, dev_ts = separate_data(dev_raw_data, 'train')\n",
    "test_msgtxt, test_titletxt, test_subreddits, test_features, test_ts = separate_data(test_raw_data, 'test')\n",
    "\n",
    "\n",
    "\n",
    "print num_train\n",
    "print len(train_target)\n",
    "print len(test_msgtxt)\n",
    "\n",
    "pprint(data_elements)\n",
    "pprint(train_features[0])\n",
    "pprint(train_ts[0])\n",
    "pprint(train_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer()\n",
    "    #vectorizer_train = TfidfVectorizer()\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    \n",
    "    '''\n",
    "    # QUARANTINE - SEE ALTERNTE BELOW (cleaner)\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(vocabulary=vocab_train)\n",
    "    #vectorizer_dev = TfidfVectorizer(vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    '''\n",
    "    # transform the dev data using the same vocab\n",
    "    v_data_dev = vectorizer_train.transform(dev_data)     # 'transform' function will preserve previous vocab\n",
    "\n",
    "    return v_data_train, v_data_dev, vocab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_bigram(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer(ngram_range=(2,2))\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(ngram_range=(2,2),vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    return v_data_train, v_data_dev, vocab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get attributes using nnz and shape\n",
    "def vectorizer_attrs(v_data):\n",
    "    nonzero = v_data.nnz\n",
    "    examples = v_data.shape[0]\n",
    "    distinct_words = v_data.shape[1]\n",
    "    avg_nonzero = float(nonzero)/examples\n",
    "    total_entries = examples*distinct_words\n",
    "    pct_nz_entries = float(nonzero)/total_entries*100\n",
    "    return \"Vocabulary size: \" + str(distinct_words) + \"\\nAverage non-zero features per example: \" + str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_reg(train_data,train_label,dev_data):\n",
    "    #lor = LogisticRegression(penalty='l1',C=1) #better, but save for later\n",
    "    lor = LogisticRegression()\n",
    "    lor.fit(train_data, train_label)\n",
    "    lor_pred = lor.predict(dev_data)\n",
    "    lor_pred_pr = lor.predict_proba(dev_data)\n",
    "    allcoefs = lor.coef_.copy()\n",
    "    # Return the prediction matrix, coefficients\n",
    "    return lor_pred, lor_pred_pr, allcoefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topn(top_n,lorcoefs,vocab):\n",
    "    allcoefs = lorcoefs.copy()\n",
    "    lbls=allcoefs.shape[0]\n",
    "    index=[]\n",
    "    words=[]\n",
    "    for num in range(top_n):\n",
    "        mxindex = allcoefs.argmax(axis=1)\n",
    "        for lbl in range(lbls):\n",
    "            allcoefs[lbl][mxindex[lbl]] = 0\n",
    "            index.append(mxindex[lbl])\n",
    "            words.append(vocab[mxindex[lbl]])\n",
    "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
    "    coefs=np.zeros((len(index),lbls))\n",
    "    for lbl in range(lbls):\n",
    "        for element in range(len(index)):\n",
    "            coefs[element][lbl] = lorcoefs[lbl][index[element]]\n",
    "    return words, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vectorized_logreg(train,train_target,test):\n",
    "    train_vdata, dev_vdata, vocab = vectorize(train,test)\n",
    "    prediction, predict_pr, allcoefs = log_reg(train_vdata,train_target,dev_vdata) #Where did train_target come from? I had to change funciton input to run on full data set\n",
    "    words, coefs = get_topn(10,allcoefs, vocab)\n",
    "    print vectorizer_attrs(train_vdata)\n",
    "    \n",
    "    return prediction #NOTE! I changed the return value to only return prediction. Sorry if this broke something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10576\n",
      "Average non-zero features per example: 53.4\n",
      "Fraction of non-zero entries in the matrix is 161880/32045280 (0.51%)\n",
      "\n",
      "Baseline model (always predicts no pizza)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      1.00      0.86       765\n",
      "          1       0.00      0.00      0.00       245\n",
      "\n",
      "avg / total       0.57      0.76      0.65      1010\n",
      "\n",
      "\n",
      "Simple Logistic Regression model w/ Count Vectorizer, no regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.85      0.81       765\n",
      "          1       0.34      0.24      0.29       245\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "Vocabulary size: 12317\n",
      "Average non-zero features per example: 53.6\n",
      "Fraction of non-zero entries in the matrix is 216394/49760680 (0.43%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACTUAL RUN CODE \n",
    "\n",
    "#Make prediction of dev data using msg text only. \n",
    "msg_pred = get_vectorized_logreg(train_msgtxt, train_target, dev_msgtxt)\n",
    "\n",
    "#Make baseline model prediction that simply predicts the most common class (no pizza) at all times\n",
    "baseline = [0]*len(msg_pred)\n",
    "\n",
    "#Compare models\n",
    "print \"\\nBaseline model (always predicts no pizza)\"\n",
    "print metrics.classification_report(dev_target,baseline)\n",
    "print \"\\nSimple Logistic Regression model w/ Count Vectorizer, no regularization\"\n",
    "print metrics.classification_report(dev_target,msg_pred)\n",
    "\n",
    "#Now make predictions on full dataset\n",
    "msg_pred_test = get_vectorized_logreg(train_msgtxt+dev_msgtxt, train_target+dev_target, test_msgtxt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     request_id  requester_received_pizza\n",
      "0      t3_i8iy4                         0\n",
      "1     t3_1mfqi0                         0\n",
      "2      t3_lclka                         0\n",
      "3     t3_1jdgdj                         0\n",
      "4      t3_t2qt4                         0\n",
      "5      t3_pvojb                         0\n",
      "6     t3_142n4c                         0\n",
      "7     t3_17rja6                         0\n",
      "8     t3_1lg6u2                         1\n",
      "9     t3_1b0mtx                         0\n",
      "10     t3_mmivq                         0\n",
      "11    t3_11ux2a                         0\n",
      "12     t3_klige                         0\n",
      "13    t3_1iv3cj                         0\n",
      "14    t3_13dh0w                         0\n",
      "15     t3_li326                         0\n",
      "16     t3_xww8y                         0\n",
      "17     t3_kogen                         1\n",
      "18     t3_ik8w1                         0\n",
      "19     t3_j3nnl                         0\n",
      "20     t3_j6xvd                         0\n",
      "21    t3_1lsrj5                         0\n",
      "22     t3_idld7                         0\n",
      "23     t3_y6amf                         0\n",
      "24     t3_hl0ft                         0\n",
      "25     t3_jg6yk                         0\n",
      "26    t3_1k5ss6                         0\n",
      "27     t3_u6i4z                         0\n",
      "28     t3_ltuhq                         0\n",
      "29    t3_1m2om7                         0\n",
      "...         ...                       ...\n",
      "1601   t3_smcni                         1\n",
      "1602   t3_zzsho                         0\n",
      "1603   t3_pr3p3                         0\n",
      "1604  t3_1hluyc                         0\n",
      "1605   t3_k025c                         0\n",
      "1606  t3_1go8c1                         0\n",
      "1607   t3_ihhbr                         0\n",
      "1608  t3_1ckfzz                         0\n",
      "1609  t3_1l7ksf                         0\n",
      "1610   t3_vawcb                         0\n",
      "1611   t3_mody8                         0\n",
      "1612  t3_1foy3z                         1\n",
      "1613   t3_qzu4g                         0\n",
      "1614   t3_lmb89                         0\n",
      "1615  t3_1m53fu                         0\n",
      "1616  t3_10cy64                         0\n",
      "1617   t3_m7m7l                         0\n",
      "1618  t3_1lfrlc                         0\n",
      "1619   t3_iihye                         1\n",
      "1620   t3_xn5hp                         1\n",
      "1621   t3_ro9ud                         0\n",
      "1622   t3_kx1pn                         0\n",
      "1623  t3_1cn0h3                         0\n",
      "1624   t3_zq12j                         0\n",
      "1625  t3_1br2hm                         0\n",
      "1626   t3_knttk                         0\n",
      "1627  t3_11wza2                         0\n",
      "1628   t3_iwbsf                         0\n",
      "1629   t3_nys7g                         1\n",
      "1630  t3_17pmtu                         0\n",
      "\n",
      "[1631 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#FAIRLY RIDICULOUS WORKFLOW- I RELOAD THE TEST DATA IN PANDAS\n",
    "'''\n",
    "This cell outputs test data to format for submission to Kaggle\n",
    "'''\n",
    "\n",
    "\n",
    "###LOAD IN DATA\n",
    "with open('test.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "\n",
    "df = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "#extract request_id so we can match against predictions for submission to kaggle\n",
    "req = df['request_id']\n",
    "##print req\n",
    "\n",
    "#make prediction in previous cell into a pandas series\n",
    "test_pred_series = pd.Series(msg_pred_test,name=\"requester_received_pizza\")\n",
    "##print test_pred_series\n",
    "\n",
    "#now join into data frame\n",
    "out = pd.concat([req,test_pred_series], axis=1)\n",
    "print out\n",
    "\n",
    "#write data frame to csv (using kaggles sample submission csv for format)\n",
    "out.to_csv('submit_to_kaggle.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3832\n",
      "Average non-zero features per example: 11.3\n",
      "Fraction of non-zero entries in the matrix is 34173/11610960 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "title_prob = get_vectorized_logreg(train_titletxt, dev_titletxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['10', '05', '18']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79120797  0.20879203]\n",
      " [ 0.76458161  0.23541839]\n",
      " [ 0.79120797  0.20879203]\n",
      " ..., \n",
      " [ 0.77608364  0.22391636]\n",
      " [ 0.78600322  0.21399678]\n",
      " [ 0.78588184  0.21411816]]\n",
      "[[ 0.75388208  0.24611792]\n",
      " [ 0.78420871  0.21579129]\n",
      " [ 0.76309099  0.23690901]\n",
      " ..., \n",
      " [ 0.75574299  0.24425701]\n",
      " [ 0.75759436  0.24240564]\n",
      " [ 0.75943616  0.24056384]]\n"
     ]
    }
   ],
   "source": [
    "feature_prediction, feature_predict_pr, feature_allcoefs = log_reg(train_features,train_target,dev_features)\n",
    "ts_prediction, ts_predict_pr, ts_allcoefs = log_reg(train_ts,train_target,dev_ts)\n",
    "print feature_predict_pr\n",
    "print ts_predict_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_array=[]\n",
    "predict_score=[]\n",
    "score_val=0.1\n",
    "prediction=[]\n",
    "for i in range(len(dev_features)):\n",
    "    proba=[]\n",
    "    proba.append(msg_prob[i])\n",
    "    proba.append(title_prob[i])\n",
    "    proba.append(feature_predict_pr[i,1])\n",
    "    proba.append(ts_predict_pr[i,1])\n",
    "    prediction_array.append(proba)\n",
    "    predict_score.append(sum(proba))\n",
    "    if sum(proba)>=score_val:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of final model is 0.413\n"
     ]
    }
   ],
   "source": [
    "print \"F1 score of final model is \" + str(round(metrics.f1_score(dev_target,prediction),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print prediction\n",
    "print dev_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
