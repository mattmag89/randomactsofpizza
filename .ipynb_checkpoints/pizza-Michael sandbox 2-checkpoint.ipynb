{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Shuffle data\n",
    "\n",
    "*transform and not fit_transform\n",
    "\n",
    "*Where did train_target come from? in In [11]. I changed code to provide explicit reference at input\n",
    "\n",
    "*What's the idea behind makign train_data 3x the size of dev_data? I know train is usually bigger but just wondering if you have a specific idea   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Import all libraries that will be needed throughout document\n",
    "'''\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "1010\n",
      "1010\n",
      "1010\n",
      "1010\n",
      "1631\n",
      "['request_month',\n",
      " 'request_day_of_month',\n",
      " 'request_hour',\n",
      " 'requester_days_since_first_post_on_raop_at_request',\n",
      " 'requester_account_age_in_days_at_request',\n",
      " 'requester_number_of_posts_on_raop_at_request',\n",
      " 'requester_number_of_posts_at_request',\n",
      " 'requester_upvotes_plus_downvotes_at_request',\n",
      " 'requester_number_of_comments_at_request',\n",
      " 'requester_upvotes_minus_downvotes_at_request',\n",
      " 'requester_number_of_comments_in_raop_at_request',\n",
      " 'requester_number_of_subreddits_at_request']\n",
      "12\n",
      "12\n",
      "[4, 24, 9, 0.0, 103.10547453703704, 0, 2, 24, 3, 18, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Load data into working space.\n",
    "The data created here should never be edited - if you want to modify (e.g. trim features) make a new copy in later cells\n",
    "'''\n",
    "\n",
    "#file names of data (must be in same folder as this notebook)\n",
    "train_dataset = 'train.json'\n",
    "test_dataset = 'test.json'\n",
    "\n",
    "def separate_data(dataset,data_type):\n",
    "    data_features=[]\n",
    "    data_msgtxt=[]\n",
    "    data_titletxt=[]\n",
    "    data_subreddits=[]\n",
    "    data_month=[]\n",
    "    data_dom=[]\n",
    "    data_hour=[]\n",
    "    \n",
    "    # Create empty array for labels\n",
    "    if data_type == 'train':\n",
    "        data_labels=[]\n",
    "   \n",
    "    # Loop over the requests\n",
    "    for request in dataset:\n",
    "        \n",
    "        # Get label data (if this is training data)\n",
    "        if data_type == 'train':\n",
    "            if request['requester_received_pizza'] == True:\n",
    "                data_labels.append(1)\n",
    "            else:\n",
    "                data_labels.append(0)\n",
    "                \n",
    "        parts = []\n",
    "        \n",
    "        # Pull out text fields\n",
    "        data_msgtxt.append(request['request_text_edit_aware'])\n",
    "        data_titletxt.append(request['request_title'])\n",
    "        data_subreddits.append(request['requester_subreddits_at_request'])\n",
    "        \n",
    "        # Pull out relevant time info\n",
    "        month = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%m\")\n",
    "        day_of_month = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%d\")\n",
    "        hour = datetime.datetime.fromtimestamp(request['unix_timestamp_of_request']).strftime(\"%H\")\n",
    "        parts.append(int(month))\n",
    "        parts.append(int(day_of_month))\n",
    "        parts.append(int(hour))\n",
    "        \n",
    "        # Add each data element to the features set\n",
    "        for element in data_elements:\n",
    "            parts.append(request[element])\n",
    "        \n",
    "        data_features.append(parts)\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        return data_msgtxt, data_titletxt, data_subreddits, data_features, data_labels\n",
    "    else:\n",
    "        return data_msgtxt, data_titletxt, data_subreddits, data_features\n",
    "\n",
    "#Load data from JSON    \n",
    "train_data = json.loads(open(train_dataset).read())\n",
    "test_data = json.loads(open(test_dataset).read())\n",
    "\n",
    "#Row titles\n",
    "data_elements=['requester_days_since_first_post_on_raop_at_request', 'requester_account_age_in_days_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_number_of_comments_at_request', 'requester_upvotes_minus_downvotes_at_request','requester_number_of_comments_in_raop_at_request','requester_number_of_subreddits_at_request']\n",
    "\n",
    "# Set np seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# List is mutable, this shuffles permanently\n",
    "np.random.shuffle(train_data) \n",
    "\n",
    "# Split data into train, dev, test\n",
    "nTrain = len(train_data)\n",
    "train_raw_data=train_data[nTrain/4:]\n",
    "dev_raw_data=train_data[:nTrain/4]\n",
    "test_raw_data = test_data\n",
    "\n",
    "# Split each dataset into msgtxt, titletxt, subreddits, features, and labels\n",
    "train_msgtxt, train_titletxt, train_subreddits, train_features, train_labels = separate_data(train_raw_data,'train')\n",
    "dev_msgtxt, dev_titletxt, dev_subreddits, dev_features, dev_labels = separate_data(dev_raw_data, 'train')\n",
    "test_msgtxt, test_titletxt, test_subreddits, test_features = separate_data(test_raw_data, 'test')\n",
    "\n",
    "# Prepend timestamp data to elements list\n",
    "data_elements.insert(0, 'request_hour')\n",
    "data_elements.insert(0, 'request_day_of_month')\n",
    "data_elements.insert(0, 'request_month')\n",
    "\n",
    "print len(train_labels)\n",
    "print len(dev_labels)\n",
    "print len(dev_subreddits)\n",
    "print len(dev_msgtxt)\n",
    "print len(dev_labels)\n",
    "print len(test_subreddits)\n",
    "\n",
    "pprint(data_elements)\n",
    "print len(data_elements)\n",
    "print len(train_features[0])\n",
    "print train_features[0] #this is a list of examples, with each element of list being the features. WIll get annoying to use if you want to just use a couple of features for a certain classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Load data from JSON\\ntrain_data = json.loads(open(train_dataset).read())\\ntest_data = json.loads(open(test_dataset).read())\\n\\n#Row titles\\ndata_elements=[u'requester_days_since_first_post_on_raop_at_request', u'requester_account_age_in_days_at_request', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_at_request', u'requester_upvotes_plus_downvotes_at_request', u'requester_number_of_comments_at_request', u'requester_upvotes_minus_downvotes_at_request',u'requester_number_of_comments_in_raop_at_request',u'requester_number_of_subreddits_at_request']\\n\\n# Set np seed\\nnp.random.seed(0)\\n\\n#Shuffle and split\\nnp.random.shuffle(train_data) #list is mutable, this shuffles permanently\\nnum_train = len(train_data)\\ntrain_raw_data=train_data[num_train/4:] #train is 3x bigger than dev\\ndev_raw_data=train_data[:num_train/4]\\ntest_raw_data = test_data\\n\\ntrain_target, train_msgtxt, train_titletxt, train_subreddits, train_features, train_ts = separate_data(train_raw_data,'train')\\ndev_target, dev_msgtxt, dev_titletxt, dev_subreddits, dev_features, dev_ts = separate_data(dev_raw_data, 'train')\\ntest_msgtxt, test_titletxt, test_subreddits, test_features, test_ts = separate_data(test_raw_data, 'test')\\n\\n\\n\\nprint num_train\\nprint len(train_target)\\nprint len(test_msgtxt)\\n\\npprint(data_elements)\\npprint(train_features[0])\\npprint(train_ts[0])\\npprint(train_target[0])\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Load data from JSON\n",
    "train_data = json.loads(open(train_dataset).read())\n",
    "test_data = json.loads(open(test_dataset).read())\n",
    "\n",
    "#Row titles\n",
    "data_elements=[u'requester_days_since_first_post_on_raop_at_request', u'requester_account_age_in_days_at_request', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_at_request', u'requester_upvotes_plus_downvotes_at_request', u'requester_number_of_comments_at_request', u'requester_upvotes_minus_downvotes_at_request',u'requester_number_of_comments_in_raop_at_request',u'requester_number_of_subreddits_at_request']\n",
    "\n",
    "# Set np seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#Shuffle and split\n",
    "np.random.shuffle(train_data) #list is mutable, this shuffles permanently\n",
    "num_train = len(train_data)\n",
    "train_raw_data=train_data[num_train/4:] #train is 3x bigger than dev\n",
    "dev_raw_data=train_data[:num_train/4]\n",
    "test_raw_data = test_data\n",
    "\n",
    "train_target, train_msgtxt, train_titletxt, train_subreddits, train_features, train_ts = separate_data(train_raw_data,'train')\n",
    "dev_target, dev_msgtxt, dev_titletxt, dev_subreddits, dev_features, dev_ts = separate_data(dev_raw_data, 'train')\n",
    "test_msgtxt, test_titletxt, test_subreddits, test_features, test_ts = separate_data(test_raw_data, 'test')\n",
    "\n",
    "\n",
    "\n",
    "print num_train\n",
    "print len(train_target)\n",
    "print len(test_msgtxt)\n",
    "\n",
    "pprint(data_elements)\n",
    "pprint(train_features[0])\n",
    "pprint(train_ts[0])\n",
    "pprint(train_target[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer()\n",
    "    #vectorizer_train = TfidfVectorizer()\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    \n",
    "    '''\n",
    "    # QUARANTINE - SEE ALTERNTE BELOW (cleaner)\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(vocabulary=vocab_train)\n",
    "    #vectorizer_dev = TfidfVectorizer(vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    '''\n",
    "    # transform the dev data using the same vocab\n",
    "    v_data_dev = vectorizer_train.transform(dev_data)     # 'transform' function will preserve previous vocab\n",
    "\n",
    "    return v_data_train, v_data_dev, vocab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_bigram(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer(ngram_range=(2,2))\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(ngram_range=(2,2),vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    return v_data_train, v_data_dev, vocab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get attributes using nnz and shape\n",
    "def vectorizer_attrs(v_data):\n",
    "    nonzero = v_data.nnz\n",
    "    examples = v_data.shape[0]\n",
    "    distinct_words = v_data.shape[1]\n",
    "    avg_nonzero = float(nonzero)/examples\n",
    "    total_entries = examples*distinct_words\n",
    "    pct_nz_entries = float(nonzero)/total_entries*100\n",
    "    return \"Vocabulary size: \" + str(distinct_words) + \"\\nAverage non-zero features per example: \" + str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_reg(train_data,train_label,dev_data):\n",
    "    #lor = LogisticRegression(penalty='l1',C=1) #better, but save for later\n",
    "    lor = LogisticRegression()\n",
    "    lor.fit(train_data, train_label)\n",
    "    lor_pred = lor.predict(dev_data)\n",
    "    lor_pred_pr = lor.predict_proba(dev_data)\n",
    "    allcoefs = lor.coef_.copy()\n",
    "    # Return the prediction matrix, coefficients\n",
    "    return lor_pred, lor_pred_pr, allcoefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topn(top_n,lorcoefs,vocab):\n",
    "    allcoefs = lorcoefs.copy()\n",
    "    lbls=allcoefs.shape[0]\n",
    "    index=[]\n",
    "    words=[]\n",
    "    for num in range(top_n):\n",
    "        mxindex = allcoefs.argmax(axis=1)\n",
    "        for lbl in range(lbls):\n",
    "            allcoefs[lbl][mxindex[lbl]] = 0\n",
    "            index.append(mxindex[lbl])\n",
    "            words.append(vocab[mxindex[lbl]])\n",
    "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
    "    coefs=np.zeros((len(index),lbls))\n",
    "    for lbl in range(lbls):\n",
    "        for element in range(len(index)):\n",
    "            coefs[element][lbl] = lorcoefs[lbl][index[element]]\n",
    "    return words, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vectorized_logreg(train,train_target,test):\n",
    "    train_vdata, dev_vdata, vocab = vectorize(train,test)\n",
    "    prediction, predict_pr, allcoefs = log_reg(train_vdata,train_target,dev_vdata) #Where did train_target come from? I had to change funciton input to run on full data set\n",
    "    words, coefs = get_topn(10,allcoefs, vocab)\n",
    "    print vectorizer_attrs(train_vdata)\n",
    "    \n",
    "    return prediction #NOTE! I changed the return value to only return prediction. Sorry if this broke something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10576\n",
      "Average non-zero features per example: 53.4\n",
      "Fraction of non-zero entries in the matrix is 161880/32045280 (0.51%)\n",
      "\n",
      "Baseline model (always predicts no pizza)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      1.00      0.86       765\n",
      "          1       0.00      0.00      0.00       245\n",
      "\n",
      "avg / total       0.57      0.76      0.65      1010\n",
      "\n",
      "\n",
      "Simple Logistic Regression model w/ Count Vectorizer, no regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.85      0.81       765\n",
      "          1       0.34      0.24      0.29       245\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "Vocabulary size: 12317\n",
      "Average non-zero features per example: 53.6\n",
      "Fraction of non-zero entries in the matrix is 216394/49760680 (0.43%)\n"
     ]
    }
   ],
   "source": [
    "#ACTUAL RUN CODE \n",
    "\n",
    "#Make prediction of dev data using msg text only. \n",
    "msg_pred = get_vectorized_logreg(train_msgtxt, train_labels, dev_msgtxt)\n",
    "\n",
    "#Make baseline model prediction that simply predicts the most common class (no pizza) at all times\n",
    "baseline = [0]*len(msg_pred)\n",
    "\n",
    "#Compare models\n",
    "print \"\\nBaseline model (always predicts no pizza)\"\n",
    "print metrics.classification_report(dev_labels,baseline)\n",
    "print \"\\nSimple Logistic Regression model w/ Count Vectorizer, no regularization\"\n",
    "print metrics.classification_report(dev_labels,msg_pred)\n",
    "\n",
    "#Now make predictions on full dataset\n",
    "msg_pred_test = get_vectorized_logreg(train_msgtxt+dev_msgtxt, train_labels+dev_labels, test_msgtxt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'N/A' u't3_i8iy4'\n",
      " u\"Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance!\"\n",
      " u'[request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado'\n",
      " 42.08386574074074 0.0 57 0 10 0 16\n",
      " [u'AskReddit', u'COents', u'Denver', u'DenverBroncos', u'LibraryofBabel', u'adventuretime', u'denvernuggets', u'fffffffuuuuuuuuuuuu', u'gaming', u'pics', u'techsupport', u'todayilearned', u'trees', u'videos', u'woahdude', u'worldnews']\n",
      " 364 840 u'j_like' 1308963419.0 1308959819.0]\n",
      "<class 'pandas.core.series.Series'>\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#MUST SET NP SEED IN THIS CELL ONCE INCORPORATED!!!\n",
    "#\n",
    "\n",
    "\n",
    "#Ridiculous work flow... load in data again!\n",
    "###LOAD IN DATA\n",
    "with open('train.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTrainRaw = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "with open('test.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTest = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "#Shuffle train data and split into train and dev\n",
    "dfTrainRaw.reindex(np.random.permutation(dfTrainRaw.index)) #shuffle\n",
    "nTrain = dfTrainRaw.shape[0]\n",
    "prop_train = 0.75 # proportion of train set to be used (remaining is dev set)\n",
    "dfTrain = dfTrainRaw[:int(nTrain*prop_train)]\n",
    "dfDev = dfTrainRaw[int(nTrain*prop_train):]\n",
    "\n",
    "#Observe\n",
    "print dfTest.values[0]\n",
    "\n",
    "req = df['request_text_edit_aware']\n",
    "print type(req)\n",
    "print type(req[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell outputs test data to format for submission to Kaggle. \n",
    "Feed it the predicited test labels data to write in below!\n",
    "'''\n",
    "\n",
    "def writeSubmission(testLabelsPredict,fileName='submit_to_kaggle.csv'):\n",
    "    '''\n",
    "    Uses dfTest dataframe, so ensure the test data hasn't been shuffled or your labels won't match the request_id's.\n",
    "    '''\n",
    "    #extract request_id so we can match against predictions for submission to kaggle\n",
    "    req = dfTest['request_id']\n",
    "\n",
    "    #make prediction in previous cell into a pandas series\n",
    "    test_pred_series = pd.Series(testLabelsPredict,name=\"requester_received_pizza\")\n",
    "    ##print test_pred_series\n",
    "\n",
    "    #now join into data frame\n",
    "    out = pd.concat([req,test_pred_series], axis=1)\n",
    "\n",
    "    #write data frame to csv (using kaggles sample submission csv for correct format)\n",
    "    out.to_csv(fileName,index=False)\n",
    "\n",
    "writeSubmission(msg_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3832\n",
      "Average non-zero features per example: 11.3\n",
      "Fraction of non-zero entries in the matrix is 34173/11610960 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "title_prob = get_vectorized_logreg(train_titletxt, dev_titletxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['10', '05', '18']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79120797  0.20879203]\n",
      " [ 0.76458161  0.23541839]\n",
      " [ 0.79120797  0.20879203]\n",
      " ..., \n",
      " [ 0.77608364  0.22391636]\n",
      " [ 0.78600322  0.21399678]\n",
      " [ 0.78588184  0.21411816]]\n",
      "[[ 0.75388208  0.24611792]\n",
      " [ 0.78420871  0.21579129]\n",
      " [ 0.76309099  0.23690901]\n",
      " ..., \n",
      " [ 0.75574299  0.24425701]\n",
      " [ 0.75759436  0.24240564]\n",
      " [ 0.75943616  0.24056384]]\n"
     ]
    }
   ],
   "source": [
    "feature_prediction, feature_predict_pr, feature_allcoefs = log_reg(train_features,train_target,dev_features)\n",
    "ts_prediction, ts_predict_pr, ts_allcoefs = log_reg(train_ts,train_target,dev_ts)\n",
    "print feature_predict_pr\n",
    "print ts_predict_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_array=[]\n",
    "predict_score=[]\n",
    "score_val=0.1\n",
    "prediction=[]\n",
    "for i in range(len(dev_features)):\n",
    "    proba=[]\n",
    "    proba.append(msg_prob[i])\n",
    "    proba.append(title_prob[i])\n",
    "    proba.append(feature_predict_pr[i,1])\n",
    "    proba.append(ts_predict_pr[i,1])\n",
    "    prediction_array.append(proba)\n",
    "    predict_score.append(sum(proba))\n",
    "    if sum(proba)>=score_val:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of final model is 0.413\n"
     ]
    }
   ],
   "source": [
    "print \"F1 score of final model is \" + str(round(metrics.f1_score(dev_target,prediction),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
