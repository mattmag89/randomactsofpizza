{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "    \n",
    "14Apr15: *Changed Model's 'test' method to take the type of report as input. We should be using AUC most of the time (that is how the comp is scored) but might be useful to use classificaition report sometimes.\n",
    "*Classes start with caps, functions/methods in camel case\n",
    "\n",
    "TODO:\n",
    "*Add getter to \"model\" for classification results\n",
    "*Fix init (Mike to do) to make pythonic. Also have it accept params so our classes can be used for grid searching WITHOUT breaking existing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Import all libraries that will be needed throughout document\n",
    "'''\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 3030, 1010)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Data_handler():\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        dfTrain_full = pd.DataFrame()\n",
    "        dfTest = pd.DataFrame()\n",
    "        dfTrain = pd.DataFrame()\n",
    "        dfDev = pd.DataFrame()\n",
    "        trainColumnNames = []\n",
    "        testColumnNames = []\n",
    "    \n",
    "    def initialize_data(self,train_json, test_json, prop_train):\n",
    "        #Load in data as panda dataframes\n",
    "        with open(train_json,'r') as fp: \n",
    "            json_data = json.load(fp)\n",
    "        self.dfTrain_full = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "        with open(test_json,'r') as fp: \n",
    "            json_data = json.load(fp)\n",
    "        self.dfTest = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "        # Set np seed\n",
    "        np.random.seed(0)\n",
    "\n",
    "        #Shuffle train data and split into train and dev\n",
    "        self.dfTrain_full.reindex(np.random.permutation(self.dfTrain_full.index)) #shuffle\n",
    "        nTrain_full = self.dfTrain_full.shape[0]\n",
    "        self.dfTrain = self.dfTrain_full[:int(nTrain_full*prop_train)]\n",
    "        self.dfDev = self.dfTrain_full[int(nTrain_full*prop_train):]\n",
    "        \n",
    "        #Save number of observations in train and dev\n",
    "        nTrain = self.dfTrain.shape[0]\n",
    "        nDev = self.dfDev.shape[0]\n",
    "\n",
    "        #Save column names for reference\n",
    "        self.trainColumnNames = self.dfTrain.columns.tolist()\n",
    "        self.testColumnNames = self.dfTest.columns.tolist() #Note test features is only a subset!\n",
    "        \n",
    "        return nTrain_full, nTrain, nDev ################# WHY DOES THIS GET RETURNED? WHAT IS IT USED FOR?\n",
    "    \n",
    "    def getTrainFull(self):\n",
    "        return self.dfTrain_full   \n",
    "    def getTest(self):\n",
    "        return self.dfTest   \n",
    "    def getTrain(self):\n",
    "        return self.dfTrain   \n",
    "    def getDev(self):\n",
    "        return self.dfDev    \n",
    "    def getTrainColumnNames(self):\n",
    "        return self.trainColumnNames   \n",
    "    def getTestColumnNames(self):\n",
    "        return self.testColumnNames\n",
    "    \n",
    "# Load in our data to master_data. Proportion in train vs dev is set here. Changing it will take effect throughout everything else in the script (the magic of classes!)\n",
    "master_data = Data_handler()\n",
    "master_data.initialize_data('train.json', 'test.json',.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer():\n",
    "    '''\n",
    "    Module that contains some of our feature engineering methods. \n",
    "    Does not contain instance variables!\n",
    "    Outputs np arrays or dataframes as needed\n",
    "    '''\n",
    "    def separateTimestamp(self,df):\n",
    "        '''\n",
    "        separates time stamp (UTC) into month, day, hour. If user's local time is of interest, \n",
    "        use the non UTC data\n",
    "        input: any dataFrame containing the timestamp data\n",
    "        '''\n",
    "\n",
    "        timeStamps = df['unix_timestamp_of_request_utc'].values #numpy array of timestamps\n",
    "        timeStampsSeparate = [] #init new\n",
    "\n",
    "        # Loop over timestamps\n",
    "        for ts in timeStamps:\n",
    "            # Pull out relevant time info\n",
    "            d = datetime.datetime.fromtimestamp(ts)\n",
    "            month = d.strftime(\"%m\")\n",
    "            day_of_month = d.strftime(\"%d\")\n",
    "            hour = d.strftime(\"%H\")\n",
    "            # Append to results\n",
    "            timeStampsSeparate.append([int(month),int(day_of_month),int(hour)])\n",
    "\n",
    "        #convert from python list to ndarray\n",
    "        return np.asarray(timeStampsSeparate)\n",
    "\n",
    "    def newTimeInfo(self,df):\n",
    "        '''\n",
    "        New expressions of time e.g day of week\n",
    "        '''\n",
    "        \n",
    "        timeStamps = df['unix_timestamp_of_request'].values #LOCAL time\n",
    "        timeStampsSeparate = [] #init new\n",
    "\n",
    "        # Loop over timestamps\n",
    "        for ts in timeStamps:\n",
    "            # Pull out relevant time info\n",
    "            d = datetime.datetime.fromtimestamp(ts)\n",
    "            day_of_week = d.isoweekday()\n",
    "            local_time =d.strftime(\"%H\")\n",
    "            # Append to results\n",
    "            timeStampsSeparate.append([int(day_of_week),int(local_time)])\n",
    "        \n",
    "        #convert from python list to ndarray\n",
    "        return np.asarray(timeStampsSeparate)\n",
    "        \n",
    "    def selectedNumericFeatures(self,df):\n",
    "        '''\n",
    "        Some hand picked numeric features that seem to work well in logistic regression\n",
    "        Dependency: separateTimestamp\n",
    "        '''\n",
    "        colNames = [master_data.getTestColumnNames()[i] for i in [5,7,8,9]] #hand picked to be plausible, not optimal \n",
    "        #numeric data\n",
    "        dfNum = df[colNames].values\n",
    "        #convert time stamp into nice format\n",
    "        dfTime = self.separateTimestamp(df)\n",
    "        #merge\n",
    "        combinedData = np.column_stack((dfNum,dfTime))\n",
    "        return combinedData\n",
    "    \n",
    "    def allNumericFeatures(self,df):\n",
    "        '''\n",
    "        All of the numeric features that come standard.\n",
    "        Dependency: separateTimestamp\n",
    "        '''\n",
    "        colNames = [master_data.getTestColumnNames()[i] for i in [4,5,6,7,8,9,10,12,13]] \n",
    "        #numeric data\n",
    "        dfNum = df[colNames].values\n",
    "        #convert time stamp into nice format\n",
    "        dfTime = self.separateTimestamp(df)\n",
    "        #merge\n",
    "        combinedData = np.column_stack((dfNum,dfTime))\n",
    "        return combinedData\n",
    "    \n",
    "    def simpleNewFeatures(self,df):\n",
    "        '''\n",
    "        Basic self explanatory features.\n",
    "        Input: whole df\n",
    "        '''\n",
    "        \n",
    "        #From text\n",
    "        title_length = [len(t) for t in df['request_title'].values]\n",
    "        req_length = [len(t) for t in df['request_text_edit_aware'].values]\n",
    "        avg_word_length = [float(len(t))/len(t.split(' ')) for t in df['request_text_edit_aware'].values] #complexity of lang\n",
    "        title_caps_norm = [sum(1 for c in t if c.isupper())/(float(len(t))+1) for t in df['request_title'].values]\n",
    "        req_caps_norm = [sum(1 for c in t if c.isupper())/(float(len(t))+1) for t in df['request_text_edit_aware'].values]\n",
    "        #num_all_caps_words\n",
    "        #num_repeated_words #perhaps normalize and/or remove shorter words\n",
    "        \n",
    "        ###special chars in text\n",
    "        num_numbers_norm = [sum(1 for c in t if c.isdigit())/(float(len(t))+1) for t in df['request_text_edit_aware'].values]\n",
    "        num_currency_chars_norm = [sum(1 for c in t if c in '$')/(float(len(t))+1) for t in df['request_text_edit_aware'].values]\n",
    "        num_exclamation_norm = [sum(1 for c in t if c in '!')/(float(len(t))+1) for t in df['request_text_edit_aware'].values]\n",
    "        num_commas_norm = [sum(1 for c in t if c in ',')/(float(len(t))+1) for t in df['request_text_edit_aware'].values]\n",
    "        \n",
    "        ###language\n",
    "        #grammar_errors #normalized for request length\n",
    "        #spelling_errors #normalized for request length\n",
    "        \n",
    "        return np.asarray([title_length,\n",
    "                          req_length,\n",
    "                          avg_word_length,\n",
    "                          title_caps_norm,\n",
    "                          req_caps_norm,\n",
    "                          num_numbers_norm,\n",
    "                          num_currency_chars_norm,\n",
    "                          num_exclamation_norm,\n",
    "                          num_commas_norm]).T\n",
    "    \n",
    "    def augNumericFeatures(self,df):\n",
    "        f1 = self.simpleNewFeatures(df)\n",
    "        f2 = self.allNumericFeatures(df)\n",
    "        return np.column_stack((f1,f2))\n",
    "    \n",
    "    def customNGrams(self,df):\n",
    "        '''\n",
    "        N Grams built by intuition\n",
    "        '''\n",
    "        #pay_it_forward = some regex to find phrase \"pay it forward\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "fe = FeatureEngineer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3030L, 9L)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.simpleNewFeatures(master_data.getTrain()).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    if __name__ == \"__main__\":\n",
    "        train_data = pd.DataFrame\n",
    "        test_data = pd.DataFrame\n",
    "        train_labels = np.array\n",
    "        prediction = np.array\n",
    "    \n",
    "    def init_test(self):\n",
    "        self.train_data = master_data.getTrain()\n",
    "        self.test_data = master_data.getDev()\n",
    "        self.prediction = np.zeros((len(self.test_data.values))) \n",
    "        self.train_labels = self.train_data['requester_received_pizza'].values\n",
    "    \n",
    "    def mod_for_ensemble(self):\n",
    "        '''\n",
    "        Replace test data with train again. \n",
    "        Use this after init_test or init_final if you want to predict on the same data you fit with.\n",
    "        '''\n",
    "        self.test_data = self.train_data\n",
    "        self.prediction = np.zeros((len(self.test_data.values))) \n",
    "    \n",
    "    def init_final(self):\n",
    "        self.train_data = master_data.getTrainFull()\n",
    "        self.test_data = master_data.getTest()\n",
    "        self.prediction = np.zeros((len(self.test_data.values)))\n",
    "        self.train_labels = self.train_data['requester_received_pizza'].values\n",
    "    \n",
    "    def test(self,criteria=metrics.roc_auc_score):\n",
    "        try:\n",
    "            predict_labels = self.test_data['requester_received_pizza'].values\n",
    "            return criteria(predict_labels,self.prediction)\n",
    "        except:\n",
    "            return \"Failed! Did you initialize as test? What criteria did you use?\"\n",
    "    \n",
    "    def finalize(self,fileName='submit_to_kaggle.csv'):\n",
    "        try:\n",
    "            '''\n",
    "            Ensure the test data hasn't been shuffled or your labels won't match the request_id's.\n",
    "            '''\n",
    "            #extract request_id so we can match against predictions for submission to kaggle\n",
    "            req = self.test_data['request_id']\n",
    "            #make prediction into a pandas series\n",
    "            print self.prediction.astype(int)\n",
    "            pred_series = pd.Series(self.prediction.astype(int),name=\"requester_received_pizza\")\n",
    "            #now join into data frame\n",
    "            out = pd.concat([req,pred_series], axis=1)\n",
    "            #write data frame to csv (using kaggles sample submission csv for correct format)\n",
    "            out.to_csv(fileName,index=False)\n",
    "        except:\n",
    "            return \"Failed! Did you initialize as final?\"\n",
    "    \n",
    "    def getPrediction(self):\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Baseline(Model):\n",
    "    '''\n",
    "    Any model just needs to implicitly inherit the model class\n",
    "    Test this with baseline.init_test() and baseline.test()\n",
    "    Generate output with baseline.init_final() and baseline.finalize()\n",
    "    \n",
    "    This is the first model we submitted to Kaggle\n",
    "    '''\n",
    "\n",
    "    def run_model(self):\n",
    "        train = self.train_data['request_text_edit_aware'].values\n",
    "        test = self.test_data['request_text_edit_aware'].values\n",
    "        v_train, v_test = self.vectorize(train,test)\n",
    "        self.prediction = self.log_reg(v_train,v_test)\n",
    "\n",
    "    def vectorize(self,train,test):\n",
    "        # transform the train data\n",
    "        vectorizer_train = CountVectorizer()\n",
    "        v_train = vectorizer_train.fit_transform(train)\n",
    "        vocab_train = vectorizer_train.get_feature_names()\n",
    "        # transform the dev data using the same vocab\n",
    "        v_test = vectorizer_train.transform(test)     # 'transform' function will preserve previous vocab\n",
    "        return v_train, v_test\n",
    "\n",
    "    def log_reg(self,v_train,v_test):\n",
    "        lor = LogisticRegression()\n",
    "        lor.fit(v_train, self.train_labels)\n",
    "        lor_pred = lor.predict(v_test)\n",
    "        # Return the prediction matrix, coefficients\n",
    "        return lor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumericModel(Model):\n",
    "    '''\n",
    "    Simple models with numeric data straight out of df\n",
    "    '''\n",
    "    def run_model(self):\n",
    "        train = fe.augNumericFeatures(self.train_data)\n",
    "        test = fe.augNumericFeatures(self.test_data)\n",
    "        #predict\n",
    "        self.prediction = self.classify(train,test)    \n",
    "    \n",
    "    def classify(self,train,test):\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(train, self.train_labels)\n",
    "        return nb.predict(test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTreeModel(Model):\n",
    "    '''\n",
    "    Basic decision tree. Runs on numeric features\n",
    "    '''\n",
    "    def run_model(self):\n",
    "        train = fe.allNumericFeatures(self.train_data)\n",
    "        test = fe.allNumericFeatures(self.test_data)\n",
    "        #predict\n",
    "        self.prediction = self.classify(train,test)  \n",
    "    \n",
    "    def classify(self,train,test):\n",
    "        dt = DecisionTreeClassifier(criterion='entropy') #criterion can be 'gini' or 'entropy'\n",
    "        dt.fit(train, self.train_labels)\n",
    "        return dt.predict(test)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForestModel(Model):\n",
    "    '''\n",
    "    Random Forest\n",
    "    -Hypothesise that the decision function of people granting pizza is quite short, \n",
    "    maybe max of 5 steps (counting the request text as one step)\n",
    "    \n",
    "    '''\n",
    "    def run_model(self):\n",
    "        train = fe.augNumericFeatures(self.train_data)\n",
    "        test = fe.augNumericFeatures(self.test_data)\n",
    "        #predict\n",
    "        self.prediction = self.classify(train,test)  \n",
    "    \n",
    "    def classify(self,train,test):\n",
    "        rf = RandomForestClassifier(n_estimators=100,\n",
    "                                    criterion='gini')#,\n",
    "                                    #max_features=10) #criterion can be 'gini' or 'entropy'\n",
    "        rf.fit(train, self.train_labels)\n",
    "        return rf.predict(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleForest(RandomForestModel):\n",
    "    '''\n",
    "    RF that incorporates text data via votes of another classifier\n",
    "    BROKEN: Because ensemble1 init_test and doesn't ever init_final, you can't use this model's \n",
    "    init_final or finalize.\n",
    "    '''\n",
    "    def ensemble1(self):\n",
    "        e = Baseline()\n",
    "        e.init_test()\n",
    "        e.run_model()\n",
    "        #get dev data prediction\n",
    "        devPrediction = e.getPrediction()\n",
    "        #now reset internal data so we can get test data prediction\n",
    "        e.mod_for_ensemble()\n",
    "        e.run_model()\n",
    "        testPrediction = e.getPrediction()\n",
    "        return testPrediction, devPrediction\n",
    "    \n",
    "    def init_final(self):\n",
    "        raise NotImplementedError('Low level ensemble is hard coded to init_test at the moment')\n",
    "    \n",
    "    def run_model(self):\n",
    "        train1 = fe.augNumericFeatures(self.train_data)\n",
    "        test1 = fe.augNumericFeatures(self.test_data)\n",
    "        train2,test2 = self.ensemble1()\n",
    "        train = np.column_stack((train1,train2))\n",
    "        test = np.column_stack((test1,test2))\n",
    "        print train.shape\n",
    "        print train1.shape\n",
    "        print train2.shape\n",
    "        print test.shape\n",
    "        print test1.shape\n",
    "        print test2.shape\n",
    "        #predict\n",
    "        self.prediction = self.classify(train,test)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleNB(NumericModel):\n",
    "    '''\n",
    "    NB that incorporates text data via votes of another classifier\n",
    "    BROKEN: Because ensemble1 init_test and doesn't ever init_final, you can't use this model's \n",
    "    init_final or finalize.\n",
    "    '''\n",
    "    def ensemble1(self):\n",
    "        e = Baseline()\n",
    "        e.init_test()\n",
    "        e.run_model()\n",
    "        #get dev data prediction\n",
    "        devPrediction = e.getPrediction()\n",
    "        #now reset internal data so we can get test data prediction\n",
    "        e.mod_for_ensemble()\n",
    "        e.run_model()\n",
    "        testPrediction = e.getPrediction()\n",
    "        return testPrediction, devPrediction\n",
    "    \n",
    "    def init_final(self):\n",
    "        raise NotImplementedError('Low level ensemble is hard coded to init_test at the moment')\n",
    "    \n",
    "    def run_model(self):\n",
    "        train1 = fe.augNumericFeatures(self.train_data)\n",
    "        test1 = fe.augNumericFeatures(self.test_data)\n",
    "        train2,test2 = self.ensemble1()\n",
    "        train = np.column_stack((train1,train2))\n",
    "        test = np.column_stack((test1,test2))\n",
    "        print train.shape\n",
    "        print train1.shape\n",
    "        print train2.shape\n",
    "        print test.shape\n",
    "        print test1.shape\n",
    "        print test2.shape\n",
    "        #predict\n",
    "        self.prediction = self.classify(train,test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Need a 28 hour day",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-c3dc3b2474ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mStackedRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     '''\n\u001b[0;32m      3\u001b[0m     \u001b[0mStacking\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mMakes\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mcombination\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mother\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhttp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspringer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10.1007\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m\u001b[0mFBF00117832\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-c3dc3b2474ba>\u001b[0m in \u001b[0;36mStackedRegression\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhttp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspringer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10.1007\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m\u001b[0mFBF00117832\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     '''\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Need a 28 hour day\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: Need a 28 hour day"
     ]
    }
   ],
   "source": [
    "class StackedRegression():\n",
    "    '''\n",
    "    Stacking with logistic regression.\n",
    "    Makes a linear combination of outputs of other models\n",
    "    http://link.springer.com/article/10.1007%2FBF00117832\n",
    "    '''\n",
    "    raise NotImplementedError(\"Need a 28 hour day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'giver_username_if_known',\n",
      " u'number_of_downvotes_of_request_at_retrieval',\n",
      " u'number_of_upvotes_of_request_at_retrieval',\n",
      " u'post_was_edited',\n",
      " u'request_id',\n",
      " u'request_number_of_comments_at_retrieval',\n",
      " u'request_text',\n",
      " u'request_text_edit_aware',\n",
      " u'request_title',\n",
      " u'requester_account_age_in_days_at_request',\n",
      " u'requester_account_age_in_days_at_retrieval',\n",
      " u'requester_days_since_first_post_on_raop_at_request',\n",
      " u'requester_days_since_first_post_on_raop_at_retrieval',\n",
      " u'requester_number_of_comments_at_request',\n",
      " u'requester_number_of_comments_at_retrieval',\n",
      " u'requester_number_of_comments_in_raop_at_request',\n",
      " u'requester_number_of_comments_in_raop_at_retrieval',\n",
      " u'requester_number_of_posts_at_request',\n",
      " u'requester_number_of_posts_at_retrieval',\n",
      " u'requester_number_of_posts_on_raop_at_request',\n",
      " u'requester_number_of_posts_on_raop_at_retrieval',\n",
      " u'requester_number_of_subreddits_at_request',\n",
      " u'requester_received_pizza',\n",
      " u'requester_subreddits_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_retrieval',\n",
      " u'requester_upvotes_plus_downvotes_at_request',\n",
      " u'requester_upvotes_plus_downvotes_at_retrieval',\n",
      " u'requester_user_flair',\n",
      " u'requester_username',\n",
      " u'unix_timestamp_of_request',\n",
      " u'unix_timestamp_of_request_utc']\n",
      "\n",
      "\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "\n",
      "\n",
      "                             request_text_edit_aware  \\\n",
      "0  Hi I am in need of food for my 4 children we a...   \n",
      "1  I spent the last money I had on gas today. Im ...   \n",
      "2  My girlfriend decided it would be a good idea ...   \n",
      "3  It's cold, I'n hungry, and to be completely ho...   \n",
      "4  hey guys:\\n I love this sub. I think it's grea...   \n",
      "\n",
      "                                       request_title  \n",
      "0            Request Colorado Springs Help Us Please  \n",
      "1  [Request] California, No cash and I could use ...  \n",
      "2  [Request] Hungry couple in Dundee, Scotland wo...  \n",
      "3  [Request] In Canada (Ontario), just got home f...  \n",
      "4  [Request] Old friend coming to visit. Would LO...  \n",
      "\n",
      "\n",
      "<type 'numpy.ndarray'>\n",
      "(3030L, 2L)\n",
      "\n",
      "\n",
      "(3030L,)   (1010L,)   (4040L,)\n",
      "\n",
      "\n",
      "(3030L,)   (3030L,)   (3030L, 2L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTo use:\\n1) instantiate your model class\\n2) initialize it as either test or final\\n3) run it and either test or finalize it\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TUTORIAL: This cell shows how the dataframes above get accessed and turned into usable numpy arrays\n",
    "'''\n",
    "\n",
    "###Task: Extracting message text AND title text into a feature vector:\n",
    "#first find name of column by printing out the list of names\n",
    "trainColumnNames = master_data.getTrainColumnNames()\n",
    "pprint(trainColumnNames) #looks like we want 'request_text_edit_aware' and 'request_title'\n",
    "#find which number this is or manually type column name\n",
    "print '\\n'\n",
    "print trainColumnNames[7]\n",
    "print trainColumnNames[8]\n",
    "\n",
    "#two ways to get data we want:\n",
    "print '\\n'\n",
    "X_train = master_data.getTrain()[['request_text_edit_aware','request_title']] #method 1\n",
    "X_train = master_data.getTrain()[[trainColumnNames[7],trainColumnNames[8]]] #method 2\n",
    "print X_train.head() #.head() just prints the first 5 rows\n",
    "\n",
    "#The above X_train is still a pandas dataframe. Converting to numpy array for sklearn is as simple as:\n",
    "print '\\n'\n",
    "X_train = X_train.values\n",
    "print type(X_train)\n",
    "print X_train.shape\n",
    "\n",
    "#In summary (quick way):\n",
    "X_train = master_data.getTrain()[['request_text_edit_aware','request_title']].values \n",
    "\n",
    "###Task: Join 2 numpy arrays horizontally (e.g. merge train and dev for final submission)\n",
    "train_data = master_data.getTrain()['request_text_edit_aware'].values\n",
    "dev_data = master_data.getDev()['request_text_edit_aware'].values\n",
    "merged_data = np.concatenate((train_data,dev_data),axis=0)\n",
    "print '\\n'\n",
    "print train_data.shape,' ',dev_data.shape,' ',merged_data.shape\n",
    "\n",
    "###Task: Join 2 numpy arrays vertically (e.g. add a bunch of features)\n",
    "train_data1 = master_data.getTrain()['request_text_edit_aware'].values\n",
    "#now we want more features... say from some feature engineering process\n",
    "train_data2 = master_data.getTrain()['request_title'].values\n",
    "train_data_merged = np.column_stack((train_data1,train_data2)) #<---- where the action is at!\n",
    "print '\\n'\n",
    "print train_data1.shape,' ',train_data2.shape,' ',train_data_merged.shape\n",
    "\n",
    "###Task: Use the Classes\n",
    "'''\n",
    "To use:\n",
    "1) instantiate your model class\n",
    "2) initialize it as either test or final\n",
    "3) run it and either test or finalize it\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_model = Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55160457863\n"
     ]
    }
   ],
   "source": [
    "# Run test of baseline\n",
    "baseline_model.init_test()\n",
    "baseline_model.run_model()\n",
    "print baseline_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 1 0]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Run final of baseline\n",
    "baseline_model.init_final()\n",
    "baseline_model.run_model()\n",
    "print baseline_model.finalize('test_finalize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528009541451\n"
     ]
    }
   ],
   "source": [
    "#Run NumericModel\n",
    "numeric_model = NumericModel()\n",
    "numeric_model.init_test()\n",
    "numeric_model.run_model()\n",
    "print numeric_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528431729546\n"
     ]
    }
   ],
   "source": [
    "#Run test of decision tree\n",
    "decision_tree_model = DecisionTreeModel()\n",
    "decision_tree_model.init_test()\n",
    "decision_tree_model.run_model()\n",
    "print decision_tree_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516212022861\n"
     ]
    }
   ],
   "source": [
    "#Run test of random forest\n",
    "rf_model = RandomForestModel()\n",
    "rf_model.init_test()\n",
    "rf_model.run_model()\n",
    "print rf_model.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030L, 22L)\n",
      "(3030L, 21L)\n",
      "(3030L,)\n",
      "(1010L, 22L)\n",
      "(1010L, 21L)\n",
      "(1010L,)\n",
      "0.55160457863\n"
     ]
    }
   ],
   "source": [
    "#Run test of EnsembleForest\n",
    "ef_model = EnsembleForest()\n",
    "ef_model.init_test()\n",
    "ef_model.run_model()\n",
    "print ef_model.test()\n",
    "\n",
    "####### WHY THE HECK IS THIS PERFORMANCE EXACTLY THE SAME AS BASELINE MODEL. \n",
    "####### THis should totally be incorporating numerical features as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030L, 22L)\n",
      "(3030L, 21L)\n",
      "(3030L,)\n",
      "(1010L, 22L)\n",
      "(1010L, 21L)\n",
      "(1010L,)\n",
      "0.550947548407\n"
     ]
    }
   ],
   "source": [
    "#Run test of EnsembleNB\n",
    "enb_model = EnsembleForest()\n",
    "enb_model.init_test()\n",
    "enb_model.run_model()\n",
    "print enb_model.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 _ giver_username_if_known\n",
      "1 _ request_id\n",
      "2 _ request_text_edit_aware\n",
      "3 _ request_title\n",
      "4 _ requester_account_age_in_days_at_request\n",
      "5 _ requester_days_since_first_post_on_raop_at_request\n",
      "6 _ requester_number_of_comments_at_request\n",
      "7 _ requester_number_of_comments_in_raop_at_request\n",
      "8 _ requester_number_of_posts_at_request\n",
      "9 _ requester_number_of_posts_on_raop_at_request\n",
      "10 _ requester_number_of_subreddits_at_request\n",
      "11 _ requester_subreddits_at_request\n",
      "12 _ requester_upvotes_minus_downvotes_at_request\n",
      "13 _ requester_upvotes_plus_downvotes_at_request\n",
      "14 _ requester_username\n",
      "15 _ unix_timestamp_of_request\n",
      "16 _ unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENTS Cell\n",
    "Cell for all experiments that don't fit neatly elsewhere.\n",
    "\"\"\"\n",
    "\n",
    "def timeVisualizer():\n",
    "    '''\n",
    "    Visualize time related trends. Does RAOP get more or less generous over time? \n",
    "    Certain times of day? Certain days of week?\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def colNamePrint():\n",
    "    '''\n",
    "    Just print out TEST column names w indices for ref.\n",
    "    '''\n",
    "    c = master_data.getTestColumnNames()\n",
    "    for i,n in enumerate(c):\n",
    "        print i, '_',n\n",
    "    #END\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
