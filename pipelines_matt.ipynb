{
 "metadata": {
  "name": "",
  "signature": "sha256:ec1534ff385e2000c7b619469268c962edffd46ef81ab3023ce840f78c60fd94"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "This cell's function:\n",
      "Import all libraries that will be needed throughout document\n",
      "'''\n",
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import re\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "from pprint import pprint\n",
      "import datetime\n",
      "import pandas as pd\n",
      "from scipy.sparse import hstack\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion, _name_estimators\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
      "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.preprocessing import normalize, LabelEncoder\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.base import TransformerMixin, ClassifierMixin, BaseEstimator, clone\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
      "\n",
      "# Text processing libraries\n",
      "from nltk.stem import PorterStemmer\n",
      "import enchant.checker\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "# PCA / GMM\n",
      "import time\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.decomposition import NMF\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.mixture import GMM\n",
      "from matplotlib.colors import LogNorm   \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Data_handler():\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        dfTrain_full = pd.DataFrame()\n",
      "        dfTest = pd.DataFrame()\n",
      "        dfTrain = pd.DataFrame()\n",
      "        dfDev = pd.DataFrame()\n",
      "        trainColumnNames = []\n",
      "        testColumnNames = []\n",
      "    \n",
      "    def initialize_data(self,train_json, test_json, prop_train):\n",
      "        #Load in data as panda dataframes\n",
      "        with open(train_json,'r') as fp: \n",
      "            json_data = json.load(fp)\n",
      "        self.dfTrain_full = pd.io.json.json_normalize(json_data)\n",
      "\n",
      "        with open(test_json,'r') as fp: \n",
      "            json_data = json.load(fp)\n",
      "        self.dfTest = pd.io.json.json_normalize(json_data)\n",
      "\n",
      "        # Set np seed\n",
      "        np.random.seed(0)\n",
      "\n",
      "        #Shuffle train data and split into train and dev\n",
      "        self.dfTrain_full.reindex(np.random.permutation(self.dfTrain_full.index)) #shuffle\n",
      "        nTrain_full = self.dfTrain_full.shape[0]\n",
      "        self.dfTrain = self.dfTrain_full[:int(nTrain_full*prop_train)]\n",
      "        self.dfDev = self.dfTrain_full[int(nTrain_full*prop_train):]\n",
      "        \n",
      "        #Save number of observations in train and dev\n",
      "        nTrain = self.dfTrain.shape[0]\n",
      "        nDev = self.dfDev.shape[0]\n",
      "\n",
      "        #Save column names for reference\n",
      "        self.trainColumnNames = self.dfTrain.columns.tolist()\n",
      "        self.testColumnNames = self.dfTest.columns.tolist() #Note test features is only a subset!\n",
      "        \n",
      "        return nTrain_full, nTrain, nDev ################# WHY DOES THIS GET RETURNED? WHAT IS IT USED FOR?\n",
      "    \n",
      "    def getTrainFull(self):\n",
      "        return self.dfTrain_full   \n",
      "    def getTest(self):\n",
      "        return self.dfTest   \n",
      "    def getTrain(self):\n",
      "        return self.dfTrain   \n",
      "    def getDev(self):\n",
      "        return self.dfDev    \n",
      "    def getTrainColumnNames(self):\n",
      "        return self.trainColumnNames   \n",
      "    def getTestColumnNames(self):\n",
      "        return self.testColumnNames\n",
      "    \n",
      "# Load in our data to master_data. Proportion in train vs dev is set here. Changing it will take effect throughout everything else in the script (the magic of classes!)\n",
      "master_data = Data_handler()\n",
      "master_data.initialize_data('train.json', 'test.json',.75)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(4040, 3030, 1010)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BASIC NUMERIC STUFF\n",
      "\n",
      "class NumericExtractor:\n",
      "    def __init__(self, cols):\n",
      "        self.cols = cols\n",
      "\n",
      "    def transform(self, data):\n",
      "        return normalize(np.asarray(data[self.cols]))\n",
      "\n",
      "    def fit(self, *_):\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TEXT STUFF\n",
      "\n",
      "class TextExtractor:\n",
      "  def __init__(self, column):\n",
      "    self.column = column\n",
      "\n",
      "  def transform(self, data):\n",
      "    return np.asarray(data[self.column])\n",
      "\n",
      "  def fit(self, *_):\n",
      "    return self\n",
      "\n",
      "class DenseTransformer:\n",
      "    def transform(self, X, y=None, **fit_params):\n",
      "        return X.todense()\n",
      "\n",
      "    def fit_transform(self, X, y=None, **fit_params):\n",
      "        self.fit(X, y, **fit_params)\n",
      "        return self.transform(X)\n",
      "\n",
      "    def fit(self, X, y=None, **fit_params):\n",
      "        return self\n",
      "\n",
      "def better_preprocessor(s):\n",
      "        st = PorterStemmer()\n",
      "        chkr = enchant.checker.SpellChecker(\"en_US\")\n",
      "        #print s\n",
      "        ####s = strip_accents_unicode(s).lower() #make everything lowercase\n",
      "        s = re.sub('[?,-/><=+_!.:()]',' ',s) #turn a lot of punctuations into spaces\n",
      "        s = ' '.join(s.split()) #remove excess white space\n",
      "        s = \" \".join(word for word in s.split() if chkr.check(word) == True)\n",
      "        s = \" \".join(st.stem(word) for word in s.split(\" \"))\n",
      "        s = re.sub('[\\']','',s) #remove apostrophes\n",
      "        #print s\n",
      "        return s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ENSEMBLE STUFF\n",
      "\n",
      "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
      "    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    clfs : array-like, shape = [n_classifiers]\n",
      "      A list of classifiers.\n",
      "      Invoking the `fit` method on the `VotingClassifier` will fit clones\n",
      "      of those original classifiers that will be stored in the class attribute\n",
      "      `self.clfs_`.\n",
      "\n",
      "    voting : str, {'hard', 'soft'} (default='hard')\n",
      "      If 'hard', uses predicted class labels for majority rule voting.\n",
      "      Else if 'soft', predicts the class label based on the argmax of\n",
      "      the sums of the predicted probalities, which is recommended for\n",
      "      an ensemble of well-calibrated classifiers.\n",
      "\n",
      "    weights : array-like, shape = [n_classifiers], optional (default=`None`)\n",
      "      Sequence of weights (`float` or `int`) to weight the occurances of\n",
      "      predicted class labels (`hard` voting) or class probabilities\n",
      "      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : array-like, shape = [n_predictions]\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> from sklearn.naive_bayes import GaussianNB\n",
      "    >>> from sklearn.ensemble import RandomForestClassifier\n",
      "    >>> clf1 = LogisticRegression(random_state=1)\n",
      "    >>> clf2 = RandomForestClassifier(random_state=1)\n",
      "    >>> clf3 = GaussianNB()\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "    >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "    >>> eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n",
      "    >>> eclf1 = eclf1.fit(X, y)\n",
      "    >>> print(eclf1.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>> eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n",
      "    >>> eclf2 = eclf2.fit(X, y)\n",
      "    >>> print(eclf2.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>> eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n",
      "    ...                          voting='soft', weights=[2,1,1])\n",
      "    >>> eclf3 = eclf3.fit(X, y)\n",
      "    >>> print(eclf3.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>>\n",
      "    \"\"\"\n",
      "    def __init__(self, clfs, voting='hard', weights=None):\n",
      "        \n",
      "        self.clfs = clfs\n",
      "        self.named_clfs = {key:value for key,value in _name_estimators(clfs)}\n",
      "        self.voting = voting\n",
      "        self.weights = weights\n",
      "        \n",
      "\n",
      "    def fit(self, X, y):\n",
      "        \"\"\" Fit the clfs.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "\n",
      "        y : array-like, shape = [n_samples]\n",
      "            Target values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "        \"\"\"\n",
      "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
      "            raise NotImplementedError('Multilabel and multi-output'\\\n",
      "                                      ' classification is not supported.')\n",
      "\n",
      "        if self.voting not in ('soft', 'hard'):\n",
      "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
      "                             % voting)\n",
      "\n",
      "        if self.weights and len(self.weights) != len(self.clfs):\n",
      "            raise ValueError('Number of classifiers and weights must be equal'\n",
      "                             '; got %d weights, %d clfs'\n",
      "                             % (len(self.weights), len(self.clfs)))\n",
      "\n",
      "        self.le_ = LabelEncoder()\n",
      "        self.le_.fit(y)\n",
      "        self.classes_ = self.le_.classes_\n",
      "        self.clfs_ = []\n",
      "        for clf in self.clfs:\n",
      "            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\n",
      "            self.clfs_.append(fitted_clf)\n",
      "        return self\n",
      "\n",
      "    def predict(self, X):\n",
      "        \"\"\" Predict class labels for X.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "\n",
      "        Returns\n",
      "        ----------\n",
      "        maj : array-like, shape = [n_samples]\n",
      "            Predicted class labels.\n",
      "        \"\"\"\n",
      "        if self.voting == 'soft':\n",
      "\n",
      "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
      "\n",
      "        else:  # 'hard' voting\n",
      "            predictions = self._predict(X)\n",
      "\n",
      "            maj = np.apply_along_axis(\n",
      "                                      lambda x:\n",
      "                                      np.argmax(np.bincount(x,\n",
      "                                                weights=self.weights)),\n",
      "                                      axis=1,\n",
      "                                      arr=predictions)\n",
      "\n",
      "        maj = self.le_.inverse_transform(maj)\n",
      "        return maj\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        \"\"\" Predict class probabilities for X.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "\n",
      "        Returns\n",
      "        ----------\n",
      "        avg : array-like, shape = [n_samples, n_classes]\n",
      "            Weighted average probability for each class per sample.\n",
      "        \"\"\"\n",
      "        avg = np.average(self._predict_probas(X), axis=0, weights=self.weights)\n",
      "        return avg\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\" Return class labels or probabilities for X for each estimator.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        If `voting='soft'`:\n",
      "          array-like = [n_classifiers, n_samples, n_classes]\n",
      "            Class probabilties calculated by each classifier.\n",
      "        If `voting='hard'`:\n",
      "          array-like = [n_classifiers, n_samples]\n",
      "            Class labels predicted by each classifier.\n",
      "        \"\"\"\n",
      "        if self.voting == 'soft':\n",
      "            return self._predict_probas(X)\n",
      "        else:\n",
      "            return self._predict(X)\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        \"\"\" Return estimator parameter names for GridSearch support\"\"\"\n",
      "        if not deep:\n",
      "            return super(EnsembleClassifier, self).get_params(deep=False)\n",
      "        else:\n",
      "            out = self.named_clfs.copy()\n",
      "            for name, step in six.iteritems(self.named_clfs):\n",
      "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
      "                    out['%s__%s' % (name, key)] = value\n",
      "            return out\n",
      "\n",
      "    def _predict(self, X):\n",
      "        \"\"\" Collect results from clf.predict calls. \"\"\"\n",
      "        return np.asarray([clf.predict(X) for clf in self.clfs_]).T\n",
      "\n",
      "    def _predict_probas(self, X):\n",
      "        \"\"\" Collect results from clf.predict calls. \"\"\"\n",
      "        return np.asarray([clf.predict_proba(X) for clf in self.clfs_])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Set up a pipeline that examines message and title\n",
      "'''\n",
      "\n",
      "# Grab all the numeric columns\n",
      "numeric_columns = [master_data.getTestColumnNames()[i] for i in [4,5,6,7,8,9,10,12,13]] \n",
      "\n",
      "class ModelTransformer(TransformerMixin):\n",
      "    \"\"\" A wrapper for scikit-learn's Pipeline class that takes a classifier\n",
      "        and makes it behave like a transformer.\n",
      "    \"\"\"\n",
      "    def __init__(self, model):\n",
      "        self.model = model\n",
      "\n",
      "    def fit(self, *args, **kwargs):\n",
      "        self.model.fit(*args, **kwargs)\n",
      "        return self\n",
      "\n",
      "    def transform(self, X, **transform_params):\n",
      "        return pd.DataFrame(self.model.predict(X))\n",
      "\n",
      "msg_pipeline = Pipeline([\n",
      "  ('features', FeatureUnion([\n",
      "    ('message', Pipeline([\n",
      "      ('extract_msg', TextExtractor('request_text_edit_aware')),\n",
      "      ('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor)),\n",
      "      #('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], max_df=.45, min_df=.01)),\n",
      "      ('densify', DenseTransformer())\n",
      "    ]))\n",
      "  ])),\n",
      "  ('clf', LogisticRegression())\n",
      "])\n",
      "    \n",
      "text_pipeline = Pipeline([\n",
      "  ('features', FeatureUnion([\n",
      "    ('message', Pipeline([\n",
      "      ('extract_msg', TextExtractor('request_text_edit_aware')),\n",
      "      #('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor)),\n",
      "      ('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], max_df=.45, min_df=.01)),\n",
      "      ('densify', DenseTransformer())\n",
      "    ])),\n",
      "    ('title', Pipeline([\n",
      "      ('extract_title', TextExtractor('request_title')),\n",
      "      ('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], max_df=.4, min_df=.1)),\n",
      "      ('densify', DenseTransformer())\n",
      "    ])),\n",
      "  ])),\n",
      "  ('classifiers', FeatureUnion([\n",
      "    ('knn', ModelTransformer(KNeighborsClassifier(n_neighbors=5))),\n",
      "    ('dtr',ModelTransformer(DecisionTreeClassifier())),\n",
      "    ('rfr', ModelTransformer(RandomForestClassifier())),\n",
      "    ('lor', ModelTransformer(LogisticRegression()))\n",
      "  ])),\n",
      "  ('clf', LogisticRegression())\n",
      "])\n",
      "\n",
      "numeric_pipeline_rf = Pipeline([\n",
      "  ('features', FeatureUnion([\n",
      "    ('extract_numeric', NumericExtractor(numeric_columns)),\n",
      "  ])),\n",
      "  ('pca', PCA(n_components=3)),\n",
      "  ('rfr', RandomForestClassifier(criterion = 'entropy',n_estimators=40))\n",
      "])\n",
      "\n",
      "numeric_pipeline_dt = Pipeline([\n",
      "  ('features', FeatureUnion([\n",
      "    ('extract_numeric', NumericExtractor(numeric_columns)),\n",
      "  ])),\n",
      "  ('dtr', DecisionTreeClassifier(criterion='entropy',max_depth=5))\n",
      "])\n",
      "\n",
      "# ENTER THE NAMES OF YOUR PIPELINES HERE\n",
      "ensemble_models = [msg_pipeline, text_pipeline, numeric_pipeline_rf, numeric_pipeline_dt]\n",
      "\n",
      "def test_eclf():\n",
      "    \n",
      "    # Get the score when the pipelines are ensembled\n",
      "    \n",
      "    train_data = master_data.getTrain()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    dev_data = master_data.getDev()\n",
      "    dev_targets = dev_data['requester_received_pizza'].values\n",
      "    \n",
      "    eclf = EnsembleClassifier(ensemble_models)\n",
      "    eclf.fit(train_data, train_targets)\n",
      "    predicted_eclf = eclf.predict(dev_data)\n",
      "    #scores = cross_val_score(eclf, train_data, train_targets, cv=5, scoring='roc_auc')\n",
      "    #print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
      "    print metrics.roc_auc_score(dev_targets,predicted_eclf)\n",
      "    \n",
      "def test_pipeline(pipeline_name):\n",
      "    \n",
      "    # Get the scores separately for each pipeline\n",
      "    \n",
      "    train_data = master_data.getTrain()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    dev_data = master_data.getDev()\n",
      "    dev_targets = dev_data['requester_received_pizza'].values\n",
      "    \n",
      "    clf = pipeline_name.fit(train_data, train_targets)\n",
      "\n",
      "    predicted = clf.predict(dev_data)\n",
      "\n",
      "    print metrics.roc_auc_score(dev_targets,predicted)\n",
      "\n",
      "def final_pipeline(filename):\n",
      "    \n",
      "    # Write a result (not currently working)\n",
      "    \n",
      "    train_data = master_data.getTrainFull()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    \n",
      "    eclf = EnsembleClassifier(ensemble_models)\n",
      "    eclf.fit(train_data, train_targets)\n",
      "\n",
      "    test_data = master_data.getTest()\n",
      "    test_requests = test_data['request_id']\n",
      "    \n",
      "    predicted_eclf = eclf.predict(dev_data)\n",
      "    pred_series = pd.Series(predicted_eclf.astype(int),name=\"requester_received_pizza\")\n",
      "    out = pd.concat([test_requests,pred_series], axis=1)\n",
      "    out.to_csv(filename,index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get AUC of ensemble model\n",
      "# See for methodology http://sebastianraschka.com/Articles/2014_ensemble_classifier.html\n",
      "test_eclf()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('message', Pipeline(steps=[('extract_msg', <__main__.TextExtractor instance at 0x7fa6933cf560>), ('counts', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        d...e, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001))])\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('message', Pipeline(steps=[('extract_msg', <__main__.TextExtractor instance at 0x7fa693407518>), ('counts', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        d...e, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001))])"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('extract_numeric', <__main__.NumericExtractor instance at 0x7fa692f472d8>)],\n",
        "       transformer_weights=None)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('rfr', RandomForestClassifier(bootstrap=True, compute_importanc...les_split=2, n_estimators=40, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0))])"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('extract_numeric', <__main__.NumericExtractor instance at 0x7fa692f47248>)],\n",
        "       transformer_weights=None)), ('dtr', DecisionTreeClassifier(compute_importances=None, criterion='entropy',\n",
        "            max_depth=5, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'))])"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.502738945269"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get AUC of a pipeline on train/dev data\n",
      "test_pipeline(msg_pipeline)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.564272860166\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get AUC of each pipeline individually\n",
      "for pipe in ensemble_models:\n",
      "    print pipe\n",
      "    test_pipeline(pipe)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('message', Pipeline(steps=[('extract_msg', <__main__.TextExtractor instance at 0x7fa6917cfef0>), ('counts', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        d...e, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001))])\n",
        "0.564272860166"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('message', Pipeline(steps=[('extract_msg', <__main__.TextExtractor instance at 0x7fa6934b85a8>), ('counts', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        d...e, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001))])\n",
        "0.509961000375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('extract_numeric', <__main__.NumericExtractor instance at 0x7fa695a44638>)],\n",
        "       transformer_weights=None)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('rfr', RandomForestClassifier(bootstrap=True, compute_importanc...les_split=2, n_estimators=40, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0))])\n",
        "0.504681010507"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
        "       transformer_list=[('extract_numeric', <__main__.NumericExtractor instance at 0x7fa6917cf170>)],\n",
        "       transformer_weights=None)), ('dtr', DecisionTreeClassifier(compute_importances=None, criterion='entropy',\n",
        "            max_depth=5, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'))])\n",
        "0.510918839616\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put together a submission\n",
      "final_pipeline('eclf_final.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Random bits of leftover code\n",
      "'''\n",
      "\n",
      "somestuff = TextExtractor('request_text_edit_aware')\n",
      "mydata = somestuff.transform(train_data)\n",
      "cntv = CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor, min_df=.05, max_df=.6)\n",
      "fit_data = cntv.fit_transform(mydata)\n",
      "\n",
      "densy = DenseTransformer()\n",
      "denso = densy.transform(fit_data)\n",
      "print denso.shape\n",
      "\n",
      "somestuff = NumericExtractor(numeric_columns)\n",
      "print somestuff.transform(train_data).shape\n",
      "numerics = somestuff.transform(train_data)\n",
      "print numerics\n",
      "\n",
      "max_k=9\n",
      "pca = PCA(n_components=max_k)\n",
      "pca.fit(numerics)\n",
      "for k in range(1,max_k+1):\n",
      "    print \"The fraction of total variance explained by the first\",k,\"principal components is:\",str(round(sum(pca.explained_variance_ratio_[0:k]),4)*100),\"%\"\n",
      "    \n",
      "    \n",
      "    \n",
      "def tune_pipeline():\n",
      "    \n",
      "    # Get the optimal parameters for each pipeline\n",
      "    # DOES NOT WORK (I think this might be an unfixable problem with featureunion)\n",
      "    # Probably not worth trying to fix\n",
      "    \n",
      "    train_data = master_data.getTrain()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    \n",
      "    parameters_t = {\n",
      "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "        'counts__min_df': (0,.25),        \n",
      "        'counts__max_df': (.25,.99),\n",
      "        'clf__C': [.25,.5,.75,1,1.25,1.5,1.75,2]\n",
      "    }\n",
      "    gs_text = GridSearchCV(text_pipeline, parameters_t, n_jobs=-1, refit=True, scoring='roc_auc')\n",
      "    gs_text.fit(train_data, train_targets)\n",
      "    \n",
      "    parameters_n = {\n",
      "        'pca__n_components': range(1,9),\n",
      "        'rfr__criterion':['gini','entropy'],\n",
      "        'rfr__max_depth': range(2,7)\n",
      "    }\n",
      "    gs_num = GridSearchCV(numeric_pipeline_rf, parameters_n, n_jobs=-1, refit=True, scoring='roc_auc')\n",
      "    gs_num.fit(train_data, train_targets)\n",
      "    \n",
      "    print gs_text.best_params_\n",
      "    print gs_num.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}