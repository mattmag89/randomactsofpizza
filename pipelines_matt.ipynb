{
 "metadata": {
  "name": "",
  "signature": "sha256:b859fda630dc783c4bd3b9beecec6dd5e216f941459e79e7387db86d97b9a1ff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "This cell's function:\n",
      "Import all libraries that will be needed throughout document\n",
      "'''\n",
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import re\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "from pprint import pprint\n",
      "import datetime\n",
      "import pandas as pd\n",
      "from scipy.sparse import hstack\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
      "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
      "\n",
      "# Text processing libraries\n",
      "from nltk.stem import PorterStemmer\n",
      "import enchant.checker\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Data_handler():\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        dfTrain_full = pd.DataFrame()\n",
      "        dfTest = pd.DataFrame()\n",
      "        dfTrain = pd.DataFrame()\n",
      "        dfDev = pd.DataFrame()\n",
      "        trainColumnNames = []\n",
      "        testColumnNames = []\n",
      "    \n",
      "    def initialize_data(self,train_json, test_json, prop_train):\n",
      "        #Load in data as panda dataframes\n",
      "        with open(train_json,'r') as fp: \n",
      "            json_data = json.load(fp)\n",
      "        self.dfTrain_full = pd.io.json.json_normalize(json_data)\n",
      "\n",
      "        with open(test_json,'r') as fp: \n",
      "            json_data = json.load(fp)\n",
      "        self.dfTest = pd.io.json.json_normalize(json_data)\n",
      "\n",
      "        # Set np seed\n",
      "        np.random.seed(0)\n",
      "\n",
      "        #Shuffle train data and split into train and dev\n",
      "        self.dfTrain_full.reindex(np.random.permutation(self.dfTrain_full.index)) #shuffle\n",
      "        nTrain_full = self.dfTrain_full.shape[0]\n",
      "        self.dfTrain = self.dfTrain_full[:int(nTrain_full*prop_train)]\n",
      "        self.dfDev = self.dfTrain_full[int(nTrain_full*prop_train):]\n",
      "        \n",
      "        #Save number of observations in train and dev\n",
      "        nTrain = self.dfTrain.shape[0]\n",
      "        nDev = self.dfDev.shape[0]\n",
      "\n",
      "        #Save column names for reference\n",
      "        self.trainColumnNames = self.dfTrain.columns.tolist()\n",
      "        self.testColumnNames = self.dfTest.columns.tolist() #Note test features is only a subset!\n",
      "        \n",
      "        return nTrain_full, nTrain, nDev ################# WHY DOES THIS GET RETURNED? WHAT IS IT USED FOR?\n",
      "    \n",
      "    def getTrainFull(self):\n",
      "        return self.dfTrain_full   \n",
      "    def getTest(self):\n",
      "        return self.dfTest   \n",
      "    def getTrain(self):\n",
      "        return self.dfTrain   \n",
      "    def getDev(self):\n",
      "        return self.dfDev    \n",
      "    def getTrainColumnNames(self):\n",
      "        return self.trainColumnNames   \n",
      "    def getTestColumnNames(self):\n",
      "        return self.testColumnNames\n",
      "    \n",
      "# Load in our data to master_data. Proportion in train vs dev is set here. Changing it will take effect throughout everything else in the script (the magic of classes!)\n",
      "master_data = Data_handler()\n",
      "master_data.initialize_data('train.json', 'test.json',.75)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 244,
       "text": [
        "(4040, 3030, 1010)"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BASIC NUMERIC STUFF\n",
      "\n",
      "class NumericExtractor:\n",
      "    def __init__(self, factor):\n",
      "        self.factor = factor\n",
      "\n",
      "    def transform(self, data):\n",
      "        return self.normalize(np.asarray(data[self.factor]))\n",
      "\n",
      "    def fit(self, *_):\n",
      "        return self\n",
      "\n",
      "    def normalize(self, array):\n",
      "        return normalize(array[:,np.newaxis], axis=1).ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 317
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "somestuff = NumericExtractor('requester_number_of_posts_at_request')\n",
      "print somestuff.transform(train_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 0 ..., 0 0 0]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py:68: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
        "  np.sqrt(norms, norms)\n"
       ]
      }
     ],
     "prompt_number": 318
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "somestuff = TextExtractor('request_text_edit_aware')\n",
      "mydata = somestuff.transform(train_data)\n",
      "cntv = CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor)\n",
      "fit_data = cntv.fit_transform(mydata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 295
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "densy = Densify()\n",
      "denso = densy.transform(fit_data)\n",
      "print denso"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 309
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TEXT STUFF\n",
      "\n",
      "class TextExtractor:\n",
      "  def __init__(self, column):\n",
      "    self.column = column\n",
      "\n",
      "  def transform(self, data):\n",
      "    return np.asarray(data[self.column])\n",
      "\n",
      "  def fit(self, *_):\n",
      "    return self\n",
      "\n",
      "class DenseTransformer:\n",
      "    def transform(self, X, y=None, **fit_params):\n",
      "        return X.todense()\n",
      "\n",
      "    def fit_transform(self, X, y=None, **fit_params):\n",
      "        self.fit(X, y, **fit_params)\n",
      "        return self.transform(X)\n",
      "\n",
      "    def fit(self, X, y=None, **fit_params):\n",
      "        return self\n",
      "\n",
      "def better_preprocessor(s):\n",
      "        st = PorterStemmer()\n",
      "        chkr = enchant.checker.SpellChecker(\"en_US\")\n",
      "        #print s\n",
      "        ####s = strip_accents_unicode(s).lower() #make everything lowercase\n",
      "        s = re.sub('[?,-/><=+_!.:()]',' ',s) #turn a lot of punctuations into spaces\n",
      "        s = ' '.join(s.split()) #remove excess white space\n",
      "        s = \" \".join(word for word in s.split() if chkr.check(word) == True)\n",
      "        s = \" \".join(st.stem(word) for word in s.split(\" \"))\n",
      "        s = re.sub('[\\']','',s) #remove apostrophes\n",
      "        #print s\n",
      "        return s\n",
      "    \n",
      "def worse_preprocessor(s):\n",
      "    s = re.sub(r\"[^a-zA-Z0-9_\\.\\s]\", \"\", s).lower()\n",
      "    return s\n",
      "\n",
      "def better_tokenizer(s):\n",
      "    tokens = [word for sent in sent_tokenize(s) for word in word_tokenize(sent)]\n",
      "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
      "    tokens = [word for word in tokens if len(word) >= 2]\n",
      "\n",
      "    lmtzr = WordNetLemmatizer()\n",
      "    return [lmtzr.lemmatize(word) for word in tokens]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 320
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Set up a pipeline that examines message and title\n",
      "'''\n",
      "\n",
      "text_pipeline = Pipeline([\n",
      "  ('features', FeatureUnion([\n",
      "    ('message', Pipeline([\n",
      "      ('extract_msg', TextExtractor('request_text_edit_aware')),\n",
      "      ('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor)),\n",
      "      ('densify', DenseTransformer())\n",
      "    ])),\n",
      "    ('title', Pipeline([\n",
      "      ('extract_title', TextExtractor('request_title')),\n",
      "      ('counts', CountVectorizer(stop_words = ['pizza', 'pizzas', 'request', 'requests'], preprocessor=better_preprocessor)),\n",
      "      ('densify', DenseTransformer())\n",
      "    ])),\n",
      "    ('nposts', Pipeline([\n",
      "      ('get_nposts', NumericExtractor('requester_number_of_posts_at_request')),\n",
      "\n",
      "  ])),\n",
      "  ('clf', LogisticRegression(C = 1))\n",
      "  #('rf', RandomForestClassifier(n_estimators=80))\n",
      "])\n",
      "\n",
      "train_data = master_data.getTrain()\n",
      "train_targets = train_data['requester_received_pizza'].values\n",
      "\n",
      "text_clf = pipeline.fit_transform(train_data, train_targets)\n",
      "\n",
      "def test_pipeline():\n",
      "    train_data = master_data.getTrain()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    \n",
      "    text_clf = pipeline.fit_transform(train_data, train_targets)\n",
      "\n",
      "    dev_data = master_data.getDev()\n",
      "    dev_targets = dev_data['requester_received_pizza'].values\n",
      "    \n",
      "    predicted = pipeline.predict(dev_data)\n",
      "\n",
      "    print metrics.roc_auc_score(dev_targets,predicted)\n",
      "\n",
      "def final_pipeline(filename):\n",
      "    train_data = master_data.getTrainFull()\n",
      "    train_targets = train_data['requester_received_pizza'].values\n",
      "    \n",
      "    text_clf = pipeline.fit(train_data, train_targets)\n",
      "\n",
      "    test_data = master_data.getTest()\n",
      "    test_requests = test_data['request_id']\n",
      "    \n",
      "    predicted = text_clf.predict(test_data)\n",
      "    pred_series = pd.Series(predicted.astype(int),name=\"requester_received_pizza\")\n",
      "    out = pd.concat([test_requests,pred_series], axis=1)\n",
      "    out.to_csv(filename,index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-321-5c4cd20bbdc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'requester_received_pizza'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtext_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;31m# Convert data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dense\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# Remap output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_arrays\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0msparse_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dense'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                     raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    264\u001b[0m                                     \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                                     'convert to a dense numpy array.')\n",
        "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
       ]
      }
     ],
     "prompt_number": 321
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get AUC of the pipeline on train/dev data\n",
      "test_pipeline()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put together a submission\n",
      "final_pipeline('text_with_stop.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gs_clf.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}