{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "*using transform and not fit_transform on dev data allows you to not have to make a whole new vectorizer with supplied vocab\n",
    "*What's the idea behind makign train_data 3x the size of dev_data? I know train is usually bigger but just wondering if you have a specific idea   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Import all libraries that will be needed throughout document\n",
    "'''\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Load data into working space.\n",
    "The data created here should never be edited - if you want to modify (e.g. trim features) make a new copy in later cells\n",
    "'''\n",
    "\n",
    "#file names of data (must be in same folder as this notebook)\n",
    "train_dataset = 'train.json'\n",
    "test_dataset = 'test.json'\n",
    "\n",
    "#Load in data as panda dataframes\n",
    "with open('train.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTrainRaw = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "with open('test.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTest = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "# Set np seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#Shuffle train data and split into train and dev\n",
    "dfTrainRaw.reindex(np.random.permutation(dfTrainRaw.index)) #shuffle\n",
    "nTrain = dfTrainRaw.shape[0]\n",
    "prop_train = 0.75 # proportion of train set to be used (remaining is dev set)\n",
    "dfTrain = dfTrainRaw[:int(nTrain*prop_train)]\n",
    "dfDev = dfTrainRaw[int(nTrain*prop_train):]\n",
    "\n",
    "#Save column names for reference\n",
    "trainColumnNames = dfTrain.columns.tolist()\n",
    "testColumnNames = dfTest.columns.tolist() #Note test features is only a subset!\n",
    "\n",
    "#Observe data\n",
    "#pprint(trainColumnNames)\n",
    "#dfTrain.describe() #summary statistics of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'giver_username_if_known',\n",
      " u'number_of_downvotes_of_request_at_retrieval',\n",
      " u'number_of_upvotes_of_request_at_retrieval',\n",
      " u'post_was_edited',\n",
      " u'request_id',\n",
      " u'request_number_of_comments_at_retrieval',\n",
      " u'request_text',\n",
      " u'request_text_edit_aware',\n",
      " u'request_title',\n",
      " u'requester_account_age_in_days_at_request',\n",
      " u'requester_account_age_in_days_at_retrieval',\n",
      " u'requester_days_since_first_post_on_raop_at_request',\n",
      " u'requester_days_since_first_post_on_raop_at_retrieval',\n",
      " u'requester_number_of_comments_at_request',\n",
      " u'requester_number_of_comments_at_retrieval',\n",
      " u'requester_number_of_comments_in_raop_at_request',\n",
      " u'requester_number_of_comments_in_raop_at_retrieval',\n",
      " u'requester_number_of_posts_at_request',\n",
      " u'requester_number_of_posts_at_retrieval',\n",
      " u'requester_number_of_posts_on_raop_at_request',\n",
      " u'requester_number_of_posts_on_raop_at_retrieval',\n",
      " u'requester_number_of_subreddits_at_request',\n",
      " u'requester_received_pizza',\n",
      " u'requester_subreddits_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_retrieval',\n",
      " u'requester_upvotes_plus_downvotes_at_request',\n",
      " u'requester_upvotes_plus_downvotes_at_retrieval',\n",
      " u'requester_user_flair',\n",
      " u'requester_username',\n",
      " u'unix_timestamp_of_request',\n",
      " u'unix_timestamp_of_request_utc']\n",
      "\n",
      "\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "\n",
      "\n",
      "                             request_text_edit_aware  \\\n",
      "0  Hi I am in need of food for my 4 children we a...   \n",
      "1  I spent the last money I had on gas today. Im ...   \n",
      "2  My girlfriend decided it would be a good idea ...   \n",
      "3  It's cold, I'n hungry, and to be completely ho...   \n",
      "4  hey guys:\\n I love this sub. I think it's grea...   \n",
      "\n",
      "                                       request_title  \n",
      "0            Request Colorado Springs Help Us Please  \n",
      "1  [Request] California, No cash and I could use ...  \n",
      "2  [Request] Hungry couple in Dundee, Scotland wo...  \n",
      "3  [Request] In Canada (Ontario), just got home f...  \n",
      "4  [Request] Old friend coming to visit. Would LO...  \n",
      "\n",
      "\n",
      "<type 'numpy.ndarray'>\n",
      "(3030, 2)\n",
      "\n",
      "\n",
      "(3030,)   (1010,)   (4040,)\n",
      "\n",
      "\n",
      "(3030,)   (3030,)   (3030, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TUTORIAL: This cell shows how the dataframes above get accessed and turned into usable numpy arrays\n",
    "'''\n",
    "\n",
    "###Task: Extracting message text AND title text into a feature vector:\n",
    "#first find name of column by printing out the list of names\n",
    "pprint(trainColumnNames) #looks like we want 'request_text_edit_aware' and 'request_title'\n",
    "#find which number this is or manually type column name\n",
    "print '\\n'\n",
    "print trainColumnNames[7]\n",
    "print trainColumnNames[8]\n",
    "\n",
    "#two ways to get data we want:\n",
    "print '\\n'\n",
    "X_train = dfTrain[['request_text_edit_aware','request_title']] #method 1\n",
    "X_train = dfTrain[[trainColumnNames[7],trainColumnNames[8]]]\n",
    "print X_train.head() #.head() just prints the first 5 rows\n",
    "\n",
    "#The above X_train is still a pandas dataframe. Converting to numpy array for sklearn is as simple as:\n",
    "print '\\n'\n",
    "X_train = X_train.values\n",
    "print type(X_train)\n",
    "print X_train.shape\n",
    "\n",
    "#In summary (quick way):\n",
    "X_train = dfTrain[['request_text_edit_aware','request_title']].values \n",
    "\n",
    "###Task: Join 2 numpy arrays horizontally (e.g. merge train and dev for final submission)\n",
    "train_data = dfTrain['request_text_edit_aware'].values\n",
    "dev_data = dfDev['request_text_edit_aware'].values\n",
    "merged_data = np.concatenate((train_data,dev_data),axis=0)\n",
    "print '\\n'\n",
    "print train_data.shape,' ',dev_data.shape,' ',merged_data.shape\n",
    "\n",
    "###Task: Join 2 numpy arrays vertically (e.g. add a bunch of features)\n",
    "train_data1 = dfTrain['request_text_edit_aware'].values\n",
    "#now we want more features... say from some feature engineering process\n",
    "train_data2 = dfTrain['request_title'].values\n",
    "train_data_merged = np.column_stack((train_data1,train_data2)) #<---- where the action is at!\n",
    "print '\\n'\n",
    "print train_data1.shape,' ',train_data2.shape,' ',train_data_merged.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 15  3 ...,  8  1 15]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is for editing the format of the data for specific classifiers or experiments. \n",
    "Always create a new copy rather than change dfTrain, dfDev or dfTest.\n",
    "'''\n",
    "\n",
    "def separateTimestamp(df):\n",
    "    '''\n",
    "    separates time stamp (UTC) into month, day, hour. If user's local time is of interest, use the non UTC data\n",
    "    '''\n",
    "\n",
    "    timeStamps = df['unix_timestamp_of_request_utc'].values #numpy array of timestamps\n",
    "    timeStampsSeparate = [] #init new\n",
    "    \n",
    "    # Loop over timestamps\n",
    "    for ts in timeStamps:\n",
    "        # Pull out relevant time info\n",
    "        month = datetime.datetime.fromtimestamp(ts).strftime(\"%m\")\n",
    "        day_of_month = datetime.datetime.fromtimestamp(ts).strftime(\"%d\")\n",
    "        hour = datetime.datetime.fromtimestamp(ts).strftime(\"%H\")\n",
    "        # Append to results\n",
    "        timeStampsSeparate.append([int(month),int(day_of_month),int(hour)])\n",
    "    \n",
    "    #convert from python list to ndarray\n",
    "    return np.asarray(timeStampsSeparate)\n",
    "        \n",
    "X_train = separateTimestamp(dfTrain)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell has quick access functions for many scipy vectorizers and classifiers\n",
    "'''\n",
    "\n",
    "def vectorize(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer()\n",
    "    #vectorizer_train = TfidfVectorizer()\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    v_data_dev = vectorizer_train.transform(dev_data)     # 'transform' function will preserve previous vocab\n",
    "    return v_data_train, v_data_dev, vocab_train\n",
    "\n",
    "def vectorize_bigram(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer(ngram_range=(2,2))\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(ngram_range=(2,2),vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    return v_data_train, v_data_dev, vocab_train\n",
    "\n",
    "def vectorizer_attrs(v_data):\n",
    "    '''\n",
    "    Get attributes using nnz and shape\n",
    "    '''\n",
    "    nonzero = v_data.nnz\n",
    "    examples = v_data.shape[0]\n",
    "    distinct_words = v_data.shape[1]\n",
    "    avg_nonzero = float(nonzero)/examples\n",
    "    total_entries = examples*distinct_words\n",
    "    pct_nz_entries = float(nonzero)/total_entries*100\n",
    "    return (\"Vocabulary size: \" + str(distinct_words) + \n",
    "            \"\\nAverage non-zero features per example: \" + \n",
    "            str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + \n",
    "            str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\")\n",
    "\n",
    "def log_reg(train_data,train_label,dev_data):\n",
    "    lor = LogisticRegression()\n",
    "    lor.fit(train_data, train_label)\n",
    "    lor_pred = lor.predict(dev_data)\n",
    "    lor_pred_pr = lor.predict_proba(dev_data)\n",
    "    allcoefs = lor.coef_.copy()\n",
    "    # Return the prediction matrix, coefficients\n",
    "    return lor_pred, lor_pred_pr, allcoefs\n",
    "\n",
    "def get_topn(top_n,lorcoefs,vocab):\n",
    "    allcoefs = lorcoefs.copy()\n",
    "    lbls=allcoefs.shape[0]\n",
    "    index=[]\n",
    "    words=[]\n",
    "    for num in range(top_n):\n",
    "        mxindex = allcoefs.argmax(axis=1)\n",
    "        for lbl in range(lbls):\n",
    "            allcoefs[lbl][mxindex[lbl]] = 0\n",
    "            index.append(mxindex[lbl])\n",
    "            words.append(vocab[mxindex[lbl]])\n",
    "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
    "    coefs=np.zeros((len(index),lbls))\n",
    "    for lbl in range(lbls):\n",
    "        for element in range(len(index)):\n",
    "            coefs[element][lbl] = lorcoefs[lbl][index[element]]\n",
    "    return words, coefs\n",
    "\n",
    "def get_vectorized_logreg(train,train_labels,test):\n",
    "    train_vdata, dev_vdata, vocab = vectorize(train,test)\n",
    "    prediction, predict_pr, allcoefs = log_reg(train_vdata,train_labels,dev_vdata) #Where did train_target come from? I had to change funciton input to run on full data set\n",
    "    words, coefs = get_topn(10,allcoefs, vocab)\n",
    "    print vectorizer_attrs(train_vdata)\n",
    "    \n",
    "    return prediction #NOTE! I changed the return value to only return prediction. Sorry if this broke something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10709\n",
      "Average non-zero features per example: 53.9\n",
      "Fraction of non-zero entries in the matrix is 163228/32448270 (0.5%)\n",
      "\n",
      "Baseline model (always predicts no pizza)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      1.00      0.86       761\n",
      "       True       0.00      0.00      0.00       249\n",
      "\n",
      "avg / total       0.57      0.75      0.65      1010\n",
      "\n",
      "ROC AUC:  0.5\n",
      "\n",
      "Simple Logistic Regression model w/ Count Vectorizer, no regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.85      0.81       761\n",
      "       True       0.36      0.25      0.30       249\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "ROC AUC:  0.55160457863\n",
      "Vocabulary size: 12317\n",
      "Average non-zero features per example: 53.6\n",
      "Fraction of non-zero entries in the matrix is 216394/49760680 (0.43%)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Benchmark Model cell\n",
    "This is the first model we submitted to Kaggle\n",
    "'''\n",
    "def benchmarkModel():\n",
    "    #Need msg text only along with labels\n",
    "    train_data = dfTrain['request_text_edit_aware'].values\n",
    "    train_labels = dfTrain['requester_received_pizza'].values\n",
    "    dev_data = dfDev['request_text_edit_aware'].values\n",
    "    dev_labels = dfDev['requester_received_pizza'].values\n",
    "    test_data = dfTest['request_text_edit_aware'].values\n",
    "\n",
    "    #Make prediction of dev data using msg text only. \n",
    "    dev_labels_pred = get_vectorized_logreg(train_data, train_labels, dev_data)\n",
    "\n",
    "    #Make baseline model prediction that simply predicts the most common class (no pizza) at all times\n",
    "    baseline = [0]*len(msg_pred)\n",
    "\n",
    "    #Compare models\n",
    "    print \"\\nBaseline model (always predicts no pizza)\"\n",
    "    print metrics.classification_report(dev_labels,baseline)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,baseline)\n",
    "    \n",
    "    print \"\\nSimple Logistic Regression model w/ Count Vectorizer, no regularization\"\n",
    "    print metrics.classification_report(dev_labels,dev_labels_pred)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,dev_labels_pred)\n",
    "\n",
    "    #Now make predictions on full dataset\n",
    "    test_labels_pred = get_vectorized_logreg(np.concatenate((train_data,dev_data),axis=0),\n",
    "                                             np.concatenate((train_labels,dev_labels),axis=0),\n",
    "                                             test_data)\n",
    "    \n",
    "    return (dev_labels_pred,test_labels_pred)\n",
    "\n",
    "dev_labels_pred,test_labels_pred = benchmarkModel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Simple Logistic Regression model w/ Count Vectorizer, no regularization\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.77      0.86      0.81       761\n",
    "       True       0.34      0.22      0.27       249\n",
    "\n",
    "avg / total       0.67      0.70      0.68      1010\n",
    "\n",
    "\n",
    "\n",
    "\\Logistic Regression model of most of the numeric data, no regularization\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.76      0.99      0.86       761\n",
    "       True       0.52      0.04      0.08       249\n",
    "\n",
    "avg / total       0.70      0.75      0.67      1010\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'requester_account_age_in_days_at_request', u'requester_number_of_comments_in_raop_at_request', u'requester_number_of_posts_on_raop_at_request']\n",
      "\\Logistic Regression model of most of the numeric data, no regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.99      0.86       761\n",
      "       True       0.65      0.04      0.08       249\n",
      "\n",
      "avg / total       0.73      0.76      0.67      1010\n",
      "\n",
      "ROC AUC:  0.518146172073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([False, False, False, ..., False, False, False], dtype=bool),\n",
       " array([False, False, False, ..., False, False, False], dtype=bool))"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Simple models with numeric data\n",
    "'''\n",
    "def simpleNumericModel():\n",
    "    #numeric data straight out of df\n",
    "    #colNames = [testColumnNames[i] for i in [4,5,6,7,8,9,10]] #hand picked to be plausible \n",
    "    colNames = [testColumnNames[i] for i in [4,7,9]] #hand picked to be plausible \n",
    "\n",
    "    print colNames\n",
    "    \n",
    "    X_train_numeric = dfTrain[colNames].values\n",
    "    X_dev_numeric = dfDev[colNames].values\n",
    "    X_test_numeric = dfTest[colNames].values\n",
    "    #split time stamp\n",
    "    X_train_time = separateTimestamp(dfTrain)\n",
    "    X_dev_time = separateTimestamp(dfDev)\n",
    "    X_test_time = separateTimestamp(dfTest)\n",
    "    #merge\n",
    "    train_data = np.column_stack((X_train_numeric,X_train_time))\n",
    "    dev_data = np.column_stack((X_dev_numeric,X_dev_time))\n",
    "    test_data = np.column_stack((X_test_numeric,X_test_time))\n",
    "    #labels\n",
    "    train_labels = dfTrain['requester_received_pizza'].values\n",
    "    dev_labels = dfDev['requester_received_pizza'].values\n",
    "\n",
    "    #Simple model\n",
    "    dev_labels_pred = log_reg(train_data, train_labels, dev_data)[0]\n",
    "\n",
    "    #Results\n",
    "    print \"\\Logistic Regression model of most of the numeric data, no regularization\"\n",
    "    print metrics.classification_report(dev_labels,dev_labels_pred)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,dev_labels_pred)\n",
    "    \n",
    "    #Now make predictions on full dataset\n",
    "    test_labels_pred = log_reg(np.concatenate((train_data,dev_data),axis=0), \n",
    "                                             np.concatenate((train_labels,dev_labels),axis=0),\n",
    "                                             test_data)[0]\n",
    "    return (dev_labels_pred,test_labels_pred)\n",
    "\n",
    "simpleNumericModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text model w/ Count Vectorizer, LR w/ L1 regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.85      0.81       761\n",
      "       True       0.36      0.25      0.30       249\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "ROC AUC:  0.55160457863\n",
      "\\Logistic Regression model of most of the numeric data, LR w/ L1 regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.94      0.84       761\n",
      "       True       0.39      0.12      0.18       249\n",
      "\n",
      "avg / total       0.67      0.74      0.68      1010\n",
      "\n",
      "ROC AUC:  0.528009541451\n",
      "\\Ensemble model!\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.85      0.81       761\n",
      "       True       0.36      0.25      0.30       249\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "ROC AUC:  0.55160457863\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ensemble Cell\n",
    "Creates a new Log Reg, combining outputs of other models\n",
    "'''\n",
    "\n",
    "def textModel(p='l2',c=1):\n",
    "    '''\n",
    "    Simple-ish text model with some regularization\n",
    "    '''\n",
    "    ###TRAINING\n",
    "    train_data = dfTrain['request_text_edit_aware'].values\n",
    "    train_labels = dfTrain['requester_received_pizza'].values\n",
    "    dev_data = dfDev['request_text_edit_aware'].values\n",
    "    dev_labels = dfDev['requester_received_pizza'].values\n",
    "    test_data = dfTest['request_text_edit_aware'].values\n",
    "    \n",
    "    # transform the data\n",
    "    cv = CountVectorizer()\n",
    "    #cv = TfidfVectorizer()\n",
    "    train_data_v = cv.fit_transform(train_data)\n",
    "    # transform the dev data using the same vocab\n",
    "    dev_data_v = cv.transform(dev_data)\n",
    "    #fit classifier\n",
    "    lr = LogisticRegression(penalty=p,C=c)\n",
    "    lr.fit(train_data_v, train_labels)\n",
    "\n",
    "    #predict labels and probabilities\n",
    "    train_labels_pred = lr.predict(train_data_v)\n",
    "    train_labels_pred_pr = lr.predict_proba(train_data_v)\n",
    "    dev_labels_pred = lr.predict(dev_data_v)\n",
    "    dev_labels_pred_pr = lr.predict_proba(dev_data_v)\n",
    "\n",
    "    #Compare models\n",
    "    print \"\\nText model w/ Count Vectorizer, LR w/ L1 regularization\"\n",
    "    print metrics.classification_report(dev_labels,dev_labels_pred)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,dev_labels_pred)\n",
    "    \n",
    "    ###SUBMISSION (USE ALL DATA)\n",
    "    all_data = np.concatenate((train_data,dev_data),axis=0)\n",
    "    all_labels = np.concatenate((train_labels,dev_labels),axis=0)\n",
    "    # transform the data\n",
    "    all_data_v = cv.fit_transform(all_data)\n",
    "    # transform the test data using the same vocab\n",
    "    test_data_v = cv.transform(test_data)     # 'transform' function will preserve previous vocab\n",
    "    #fit classifier\n",
    "    lr.fit(all_data_v, all_labels)\n",
    "\n",
    "    #predict labels and probabilities of alldata (for training ensemble model for test prediction\n",
    "    all_labels_pred = lr.predict(all_data_v)\n",
    "    all_labels_pred_pr = lr.predict_proba(all_data_v)\n",
    "    test_labels_pred = lr.predict(test_data_v)\n",
    "    test_labels_pred_pr = lr.predict_proba(test_data_v)    \n",
    "    \n",
    "    #return all this for ensemble processing (just predicted labels at this stage, not probabilities)\n",
    "    return (train_labels_pred,dev_labels_pred,all_labels_pred,test_labels_pred)\n",
    "\n",
    "def numericModel(p='l2',c=0.5):\n",
    "    #numeric data straight out of df\n",
    "    colNames = [testColumnNames[i] for i in [5,7,8,9]] #hand picked to be plausible \n",
    "    X_train_numeric = dfTrain[colNames].values\n",
    "    X_dev_numeric = dfDev[colNames].values\n",
    "    X_test_numeric = dfTest[colNames].values\n",
    "    #split time stamp\n",
    "    X_train_time = separateTimestamp(dfTrain)\n",
    "    X_dev_time = separateTimestamp(dfDev)\n",
    "    X_test_time = separateTimestamp(dfTest)\n",
    "    #merge\n",
    "    train_data = np.column_stack((X_train_numeric,X_train_time))\n",
    "    dev_data = np.column_stack((X_dev_numeric,X_dev_time))\n",
    "    test_data = np.column_stack((X_test_numeric,X_test_time))\n",
    "    #labels\n",
    "    train_labels = dfTrain['requester_received_pizza'].values\n",
    "    dev_labels = dfDev['requester_received_pizza'].values\n",
    "\n",
    "    ###TRAINING\n",
    "    #fit classifier\n",
    "    #lr = LogisticRegression(penalty=p,C=c)\n",
    "    lr = GaussianNB()\n",
    "    lr.fit(train_data, train_labels)\n",
    "\n",
    "    #predict labels and probabilities\n",
    "    train_labels_pred = lr.predict(train_data)\n",
    "    train_labels_pred_pr = lr.predict_proba(train_data)\n",
    "    dev_labels_pred = lr.predict(dev_data)\n",
    "    dev_labels_pred_pr = lr.predict_proba(dev_data)\n",
    "\n",
    "    #Compare models\n",
    "    print \"\\Logistic Regression model of most of the numeric data, LR w/ L1 regularization\"\n",
    "    print metrics.classification_report(dev_labels,dev_labels_pred)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,dev_labels_pred)\n",
    "    \n",
    "    ###SUBMISSION (USE ALL DATA)\n",
    "    all_data = np.concatenate((train_data,dev_data),axis=0)\n",
    "    all_labels = np.concatenate((train_labels,dev_labels),axis=0)\n",
    "    #fit classifier\n",
    "    lr.fit(all_data, all_labels)\n",
    "\n",
    "    #predict labels and probabilities of alldata (for training ensemble model for test prediction\n",
    "    all_labels_pred = lr.predict(all_data)\n",
    "    all_labels_pred_pr = lr.predict_proba(all_data)\n",
    "    test_labels_pred = lr.predict(test_data)\n",
    "    test_labels_pred_pr = lr.predict_proba(test_data)    \n",
    "    \n",
    "    #return all this for ensemble processing (just predicted labels at this stage, not probabilities)\n",
    "    return (train_labels_pred,dev_labels_pred,all_labels_pred,test_labels_pred)\n",
    "\n",
    "def simpleEnsembleModel(models):\n",
    "    train_labels_pred_list = []\n",
    "    dev_labels_pred_list = []\n",
    "    all_labels_pred_list = []\n",
    "    test_labels_pred_list = [] #don't think this is needed\n",
    "    \n",
    "    #run all models provided and add to list of features\n",
    "    for model in models:\n",
    "        a,b,c,d = model()\n",
    "        train_labels_pred_list.append(a)\n",
    "        dev_labels_pred_list.append(b)\n",
    "        all_labels_pred_list.append(c)\n",
    "        test_labels_pred_list.append(d)\n",
    "    #concatenate features into numpy arrays\n",
    "    train_data_layer2 = np.column_stack(tuple(train_labels_pred_list))\n",
    "    dev_data_layer2 = np.column_stack(tuple(dev_labels_pred_list))\n",
    "    all_data_layer2 = np.column_stack(tuple(all_labels_pred_list))\n",
    "    test_data_layer2 = np.column_stack(tuple(test_labels_pred_list))\n",
    "    \n",
    "    ###TRAINING\n",
    "    #fit classifier\n",
    "    #lr = BernoulliNB(alpha=0.1)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_data_layer2, train_labels)\n",
    "\n",
    "    #predict labels and probabilities\n",
    "    dev_labels_pred_layer2 = lr.predict(dev_data_layer2)\n",
    "    dev_labels_pred_layer2_pr = lr.predict_proba(dev_data_layer2)\n",
    "\n",
    "    #Compare models\n",
    "    print \"\\Ensemble model!\"\n",
    "    print metrics.classification_report(dev_labels,dev_labels_pred_layer2)\n",
    "    print \"ROC AUC: \", metrics.roc_auc_score(dev_labels,dev_labels_pred_layer2)\n",
    "    \n",
    "    ###SUBMISSION (USE ALL DATA)\n",
    "    all_labels = np.concatenate((train_labels,dev_labels),axis=0)\n",
    "    #fit classifier\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(all_data_layer2, all_labels)\n",
    "\n",
    "    #predict test set labels\n",
    "    test_labels_pred = lr.predict(test_data_layer2)\n",
    "    test_labels_pred_pr = lr.predict_proba(test_data_layer2)        \n",
    "    \n",
    "    return test_labels_pred\n",
    "    \n",
    "test_labels_pred_ensemble = simpleEnsembleModel([textModel,numericModel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text model w/ Count Vectorizer, LR w/ L1 regularization\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.77      0.94      0.84       761\n",
    "       True       0.42      0.14      0.21       249\n",
    "\n",
    "avg / total       0.68      0.74      0.69      1010\n",
    "\n",
    "\\Logistic Regression model of most of the numeric data, LR w/ L1 regularization\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.76      0.99      0.86       761\n",
    "       True       0.52      0.04      0.08       249\n",
    "\n",
    "avg / total       0.70      0.75      0.67      1010\n",
    "\n",
    "\\Ensemble model!\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.77      0.92      0.84       761\n",
    "       True       0.43      0.18      0.25       249\n",
    "\n",
    "avg / total       0.69      0.74      0.70      1010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell outputs test data to format for submission to Kaggle. \n",
    "Feed it the predicited test labels data to write in below!\n",
    "'''\n",
    "\n",
    "def writeSubmission(testLabelsPredict,fileName='submit_to_kaggle.csv'):\n",
    "    '''\n",
    "    Uses dfTest dataframe, so ensure the test data hasn't been shuffled or your labels won't match the request_id's.\n",
    "    '''\n",
    "    #extract request_id so we can match against predictions for submission to kaggle\n",
    "    req = dfTest['request_id']\n",
    "\n",
    "    #make prediction in previous cell into a pandas series\n",
    "    test_pred_series = pd.Series(testLabelsPredict.astype(int),name=\"requester_received_pizza\")\n",
    "\n",
    "    #now join into data frame\n",
    "    out = pd.concat([req,test_pred_series], axis=1)\n",
    "\n",
    "    #write data frame to csv (using kaggles sample submission csv for correct format)\n",
    "    out.to_csv(fileName,index=False)\n",
    "\n",
    "writeSubmission(test_labels_pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3832\n",
      "Average non-zero features per example: 11.3\n",
      "Fraction of non-zero entries in the matrix is 34173/11610960 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "title_prob = get_vectorized_logreg(train_titletxt, dev_titletxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['10', '05', '18']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79120797  0.20879203]\n",
      " [ 0.76458161  0.23541839]\n",
      " [ 0.79120797  0.20879203]\n",
      " ..., \n",
      " [ 0.77608364  0.22391636]\n",
      " [ 0.78600322  0.21399678]\n",
      " [ 0.78588184  0.21411816]]\n",
      "[[ 0.75388208  0.24611792]\n",
      " [ 0.78420871  0.21579129]\n",
      " [ 0.76309099  0.23690901]\n",
      " ..., \n",
      " [ 0.75574299  0.24425701]\n",
      " [ 0.75759436  0.24240564]\n",
      " [ 0.75943616  0.24056384]]\n"
     ]
    }
   ],
   "source": [
    "feature_prediction, feature_predict_pr, feature_allcoefs = log_reg(train_features,train_target,dev_features)\n",
    "ts_prediction, ts_predict_pr, ts_allcoefs = log_reg(train_ts,train_target,dev_ts)\n",
    "print feature_predict_pr\n",
    "print ts_predict_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_array=[]\n",
    "predict_score=[]\n",
    "score_val=0.1\n",
    "prediction=[]\n",
    "for i in range(len(dev_features)):\n",
    "    proba=[]\n",
    "    proba.append(msg_prob[i])\n",
    "    proba.append(title_prob[i])\n",
    "    proba.append(feature_predict_pr[i,1])\n",
    "    proba.append(ts_predict_pr[i,1])\n",
    "    prediction_array.append(proba)\n",
    "    predict_score.append(sum(proba))\n",
    "    if sum(proba)>=score_val:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of final model is 0.413\n"
     ]
    }
   ],
   "source": [
    "print \"F1 score of final model is \" + str(round(metrics.f1_score(dev_target,prediction),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
