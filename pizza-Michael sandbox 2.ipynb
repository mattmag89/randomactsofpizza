{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "*using transform and not fit_transform on dev data allows you to not have to make a whole new vectorizer with supplied vocab\n",
    "*What's the idea behind makign train_data 3x the size of dev_data? I know train is usually bigger but just wondering if you have a specific idea   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Import all libraries that will be needed throughout document\n",
    "'''\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell's function:\n",
    "Load data into working space.\n",
    "The data created here should never be edited - if you want to modify (e.g. trim features) make a new copy in later cells\n",
    "'''\n",
    "\n",
    "#file names of data (must be in same folder as this notebook)\n",
    "train_dataset = 'train.json'\n",
    "test_dataset = 'test.json'\n",
    "\n",
    "#Load in data as panda dataframes\n",
    "with open('train.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTrainRaw = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "with open('test.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTest = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "# Set np seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#Shuffle train data and split into train and dev\n",
    "dfTrainRaw.reindex(np.random.permutation(dfTrainRaw.index)) #shuffle\n",
    "nTrain = dfTrainRaw.shape[0]\n",
    "prop_train = 0.75 # proportion of train set to be used (remaining is dev set)\n",
    "dfTrain = dfTrainRaw[:int(nTrain*prop_train)]\n",
    "dfDev = dfTrainRaw[int(nTrain*prop_train):]\n",
    "\n",
    "#Save column names for reference\n",
    "trainColumnNames = dfTrain.columns.tolist()\n",
    "testColumnNames = dfTest.columns.tolist() #Note test features is only a subset!\n",
    "\n",
    "#Observe data\n",
    "#pprint(trainColumnNames)\n",
    "#dfTrain.describe() #summary statistics of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'giver_username_if_known',\n",
      " u'number_of_downvotes_of_request_at_retrieval',\n",
      " u'number_of_upvotes_of_request_at_retrieval',\n",
      " u'post_was_edited',\n",
      " u'request_id',\n",
      " u'request_number_of_comments_at_retrieval',\n",
      " u'request_text',\n",
      " u'request_text_edit_aware',\n",
      " u'request_title',\n",
      " u'requester_account_age_in_days_at_request',\n",
      " u'requester_account_age_in_days_at_retrieval',\n",
      " u'requester_days_since_first_post_on_raop_at_request',\n",
      " u'requester_days_since_first_post_on_raop_at_retrieval',\n",
      " u'requester_number_of_comments_at_request',\n",
      " u'requester_number_of_comments_at_retrieval',\n",
      " u'requester_number_of_comments_in_raop_at_request',\n",
      " u'requester_number_of_comments_in_raop_at_retrieval',\n",
      " u'requester_number_of_posts_at_request',\n",
      " u'requester_number_of_posts_at_retrieval',\n",
      " u'requester_number_of_posts_on_raop_at_request',\n",
      " u'requester_number_of_posts_on_raop_at_retrieval',\n",
      " u'requester_number_of_subreddits_at_request',\n",
      " u'requester_received_pizza',\n",
      " u'requester_subreddits_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_request',\n",
      " u'requester_upvotes_minus_downvotes_at_retrieval',\n",
      " u'requester_upvotes_plus_downvotes_at_request',\n",
      " u'requester_upvotes_plus_downvotes_at_retrieval',\n",
      " u'requester_user_flair',\n",
      " u'requester_username',\n",
      " u'unix_timestamp_of_request',\n",
      " u'unix_timestamp_of_request_utc']\n",
      "\n",
      "\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "\n",
      "\n",
      "                             request_text_edit_aware  \\\n",
      "0  Hi I am in need of food for my 4 children we a...   \n",
      "1  I spent the last money I had on gas today. Im ...   \n",
      "2  My girlfriend decided it would be a good idea ...   \n",
      "3  It's cold, I'n hungry, and to be completely ho...   \n",
      "4  hey guys:\\n I love this sub. I think it's grea...   \n",
      "\n",
      "                                       request_title  \n",
      "0            Request Colorado Springs Help Us Please  \n",
      "1  [Request] California, No cash and I could use ...  \n",
      "2  [Request] Hungry couple in Dundee, Scotland wo...  \n",
      "3  [Request] In Canada (Ontario), just got home f...  \n",
      "4  [Request] Old friend coming to visit. Would LO...  \n",
      "\n",
      "\n",
      "<type 'numpy.ndarray'>\n",
      "(3030, 2)\n",
      "\n",
      "\n",
      "(3030,)   (1010,)   (4040,)\n",
      "\n",
      "\n",
      "(3030,)   (3030,)   (3030, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TUTORIAL: This cell shows how the dataframes above get accessed and turned into usable numpy arrays\n",
    "'''\n",
    "\n",
    "###Task: Extracting message text AND title text into a feature vector:\n",
    "#first find name of column by printing out the list of names\n",
    "pprint(trainColumnNames) #looks like we want 'request_text_edit_aware' and 'request_title'\n",
    "#find which number this is or manually type column name\n",
    "print '\\n'\n",
    "print trainColumnNames[7]\n",
    "print trainColumnNames[8]\n",
    "\n",
    "#two ways to get data we want:\n",
    "print '\\n'\n",
    "X_train = dfTrain[['request_text_edit_aware','request_title']] #method 1\n",
    "X_train = dfTrain[[trainColumnNames[7],trainColumnNames[8]]]\n",
    "print X_train.head() #.head() just prints the first 5 rows\n",
    "\n",
    "#The above X_train is still a pandas dataframe. Converting to numpy array for sklearn is as simple as:\n",
    "print '\\n'\n",
    "X_train = X_train.values\n",
    "print type(X_train)\n",
    "print X_train.shape\n",
    "\n",
    "#In summary (quick way):\n",
    "X_train = dfTrain[['request_text_edit_aware','request_title']].values \n",
    "\n",
    "###Task: Join 2 numpy arrays horizontally (e.g. merge train and dev for final submission)\n",
    "train_data = dfTrain['request_text_edit_aware'].values\n",
    "dev_data = dfDev['request_text_edit_aware'].values\n",
    "merged_data = np.hstack((train_data,dev_data))\n",
    "print '\\n'\n",
    "print train_data.shape,' ',dev_data.shape,' ',merged_data.shape\n",
    "\n",
    "###Task: Join 2 numpy arrays vertically (e.g. add a bunch of features)\n",
    "train_data1 = dfTrain['request_text_edit_aware'].values\n",
    "#now we want more features... say from some feature engineering process\n",
    "train_data2 = dfTrain['request_title'].values\n",
    "train_data_merged = np.column_stack((train_data1,train_data2))\n",
    "print '\\n'\n",
    "print train_data1.shape,' ',train_data2.shape,' ',train_data_merged.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  6  8]\n",
      " [ 3 25 15]\n",
      " [10 27  3]\n",
      " ..., \n",
      " [10  2  8]\n",
      " [ 6 21  1]\n",
      " [ 7 22 15]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is for editing the format of the data for specific classifiers or experiments. \n",
    "Always create a new copy rather than change dfTrain, dfDev or dfTest.\n",
    "'''\n",
    "\n",
    "def separateTimestamp(df):\n",
    "    '''\n",
    "    separates time stamp (UTC) into month, day, hour. If user's local time is of interest, use the non UTC data\n",
    "    '''\n",
    "\n",
    "    timeStamps = df['unix_timestamp_of_request_utc'].values #numpy array of timestamps\n",
    "    timeStampsSeparate = [] #init new\n",
    "    \n",
    "    # Loop over timestamps\n",
    "    for ts in timeStamps:\n",
    "        # Pull out relevant time info\n",
    "        month = datetime.datetime.fromtimestamp(ts).strftime(\"%m\")\n",
    "        day_of_month = datetime.datetime.fromtimestamp(ts).strftime(\"%d\")\n",
    "        hour = datetime.datetime.fromtimestamp(ts).strftime(\"%H\")\n",
    "        # Append to results\n",
    "        timeStampsSeparate.append([int(month),int(day_of_month),int(hour)])\n",
    "    \n",
    "    #convert from python list to ndarray\n",
    "    return np.asarray(timeStampsSeparate)\n",
    "        \n",
    "X_train = separateTimestamp(dfTrain)\n",
    "print X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell has quick access functions for many scipy vectorizers and classifiers\n",
    "'''\n",
    "\n",
    "def vectorize(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer()\n",
    "    #vectorizer_train = TfidfVectorizer()\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    v_data_dev = vectorizer_train.transform(dev_data)     # 'transform' function will preserve previous vocab\n",
    "    return v_data_train, v_data_dev, vocab_train\n",
    "\n",
    "def vectorize_bigram(train_data,dev_data):\n",
    "    # transform the train data\n",
    "    vectorizer_train = CountVectorizer(ngram_range=(2,2))\n",
    "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
    "    vocab_train = vectorizer_train.get_feature_names()\n",
    "    # transform the dev data using the same vocab\n",
    "    vectorizer_dev = CountVectorizer(ngram_range=(2,2),vocabulary=vocab_train)\n",
    "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
    "    return v_data_train, v_data_dev, vocab_train\n",
    "\n",
    "def vectorizer_attrs(v_data):\n",
    "    '''\n",
    "    Get attributes using nnz and shape\n",
    "    '''\n",
    "    nonzero = v_data.nnz\n",
    "    examples = v_data.shape[0]\n",
    "    distinct_words = v_data.shape[1]\n",
    "    avg_nonzero = float(nonzero)/examples\n",
    "    total_entries = examples*distinct_words\n",
    "    pct_nz_entries = float(nonzero)/total_entries*100\n",
    "    return (\"Vocabulary size: \" + str(distinct_words) + \n",
    "            \"\\nAverage non-zero features per example: \" + \n",
    "            str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + \n",
    "            str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\")\n",
    "\n",
    "def log_reg(train_data,train_label,dev_data):\n",
    "    #lor = LogisticRegression(penalty='l1',C=1) #better, but save for later\n",
    "    lor = LogisticRegression()\n",
    "    lor.fit(train_data, train_label)\n",
    "    lor_pred = lor.predict(dev_data)\n",
    "    lor_pred_pr = lor.predict_proba(dev_data)\n",
    "    allcoefs = lor.coef_.copy()\n",
    "    # Return the prediction matrix, coefficients\n",
    "    return lor_pred, lor_pred_pr, allcoefs\n",
    "\n",
    "def get_topn(top_n,lorcoefs,vocab):\n",
    "    allcoefs = lorcoefs.copy()\n",
    "    lbls=allcoefs.shape[0]\n",
    "    index=[]\n",
    "    words=[]\n",
    "    for num in range(top_n):\n",
    "        mxindex = allcoefs.argmax(axis=1)\n",
    "        for lbl in range(lbls):\n",
    "            allcoefs[lbl][mxindex[lbl]] = 0\n",
    "            index.append(mxindex[lbl])\n",
    "            words.append(vocab[mxindex[lbl]])\n",
    "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
    "    coefs=np.zeros((len(index),lbls))\n",
    "    for lbl in range(lbls):\n",
    "        for element in range(len(index)):\n",
    "            coefs[element][lbl] = lorcoefs[lbl][index[element]]\n",
    "    return words, coefs\n",
    "\n",
    "def get_vectorized_logreg(train,train_labels,test):\n",
    "    train_vdata, dev_vdata, vocab = vectorize(train,test)\n",
    "    prediction, predict_pr, allcoefs = log_reg(train_vdata,train_labels,dev_vdata) #Where did train_target come from? I had to change funciton input to run on full data set\n",
    "    words, coefs = get_topn(10,allcoefs, vocab)\n",
    "    print vectorizer_attrs(train_vdata)\n",
    "    \n",
    "    return prediction #NOTE! I changed the return value to only return prediction. Sorry if this broke something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10709\n",
      "Average non-zero features per example: 53.9\n",
      "Fraction of non-zero entries in the matrix is 163228/32448270 (0.5%)\n",
      "\n",
      "Baseline model (always predicts no pizza)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      1.00      0.86       761\n",
      "       True       0.00      0.00      0.00       249\n",
      "\n",
      "avg / total       0.57      0.75      0.65      1010\n",
      "\n",
      "\n",
      "Simple Logistic Regression model w/ Count Vectorizer, no regularization\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.85      0.81       761\n",
      "       True       0.36      0.25      0.30       249\n",
      "\n",
      "avg / total       0.67      0.70      0.68      1010\n",
      "\n",
      "Vocabulary size: 12317\n",
      "Average non-zero features per example: 53.6\n",
      "Fraction of non-zero entries in the matrix is 216394/49760680 (0.43%)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Benchmark Model cell\n",
    "This is the first model we submitted to Kaggle\n",
    "'''\n",
    "\n",
    "#Need msg text only along with labels\n",
    "train_data = dfTrain['request_text_edit_aware'].values\n",
    "train_labels = dfTrain['requester_received_pizza'].values\n",
    "dev_data = dfDev['request_text_edit_aware'].values\n",
    "dev_labels = dfDev['requester_received_pizza'].values\n",
    "test_data = dfTest['request_text_edit_aware'].values\n",
    "    \n",
    "#Make prediction of dev data using msg text only. \n",
    "dev_labels_pred = get_vectorized_logreg(train_data, train_labels, dev_data)\n",
    "\n",
    "#Make baseline model prediction that simply predicts the most common class (no pizza) at all times\n",
    "baseline = [0]*len(msg_pred)\n",
    "\n",
    "#Compare models\n",
    "print \"\\nBaseline model (always predicts no pizza)\"\n",
    "print metrics.classification_report(dev_labels,baseline)\n",
    "print \"\\nSimple Logistic Regression model w/ Count Vectorizer, no regularization\"\n",
    "print metrics.classification_report(dev_labels,dev_labels_pred)\n",
    "\n",
    "#Now make predictions on full dataset\n",
    "test_labels_pred = get_vectorized_logreg(np.hstack((train_data,dev_data)), np.hstack((train_labels,dev_labels)), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#MUST SET NP SEED IN THIS CELL ONCE INCORPORATED!!!\n",
    "#\n",
    "\n",
    "\n",
    "#Ridiculous work flow... load in data again!\n",
    "###LOAD IN DATA\n",
    "with open('train.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTrainRaw = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "with open('test.json','r') as fp: \n",
    "    json_data = json.load(fp)\n",
    "dfTest = pd.io.json.json_normalize(json_data)\n",
    "\n",
    "#Shuffle train data and split into train and dev\n",
    "dfTrainRaw.reindex(np.random.permutation(dfTrainRaw.index)) #shuffle\n",
    "nTrain = dfTrainRaw.shape[0]\n",
    "prop_train = 0.75 # proportion of train set to be used (remaining is dev set)\n",
    "dfTrain = dfTrainRaw[:int(nTrain*prop_train)]\n",
    "dfDev = dfTrainRaw[int(nTrain*prop_train):]\n",
    "\n",
    "#Save column names for reference\n",
    "trainColumnNames = dfTrain.columns.tolist()\n",
    "testColumnNames = dfTest.columns.tolist() #Note test features is only a subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell outputs test data to format for submission to Kaggle. \n",
    "Feed it the predicited test labels data to write in below!\n",
    "'''\n",
    "\n",
    "def writeSubmission(testLabelsPredict,fileName='submit_to_kaggle.csv'):\n",
    "    '''\n",
    "    Uses dfTest dataframe, so ensure the test data hasn't been shuffled or your labels won't match the request_id's.\n",
    "    '''\n",
    "    #extract request_id so we can match against predictions for submission to kaggle\n",
    "    req = dfTest['request_id']\n",
    "\n",
    "    #make prediction in previous cell into a pandas series\n",
    "    test_pred_series = pd.Series(testLabelsPredict,name=\"requester_received_pizza\")\n",
    "    ##print test_pred_series\n",
    "\n",
    "    #now join into data frame\n",
    "    out = pd.concat([req,test_pred_series], axis=1)\n",
    "\n",
    "    #write data frame to csv (using kaggles sample submission csv for correct format)\n",
    "    out.to_csv(fileName,index=False)\n",
    "\n",
    "writeSubmission(msg_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3832\n",
      "Average non-zero features per example: 11.3\n",
      "Fraction of non-zero entries in the matrix is 34173/11610960 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "title_prob = get_vectorized_logreg(train_titletxt, dev_titletxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['10', '05', '18']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79120797  0.20879203]\n",
      " [ 0.76458161  0.23541839]\n",
      " [ 0.79120797  0.20879203]\n",
      " ..., \n",
      " [ 0.77608364  0.22391636]\n",
      " [ 0.78600322  0.21399678]\n",
      " [ 0.78588184  0.21411816]]\n",
      "[[ 0.75388208  0.24611792]\n",
      " [ 0.78420871  0.21579129]\n",
      " [ 0.76309099  0.23690901]\n",
      " ..., \n",
      " [ 0.75574299  0.24425701]\n",
      " [ 0.75759436  0.24240564]\n",
      " [ 0.75943616  0.24056384]]\n"
     ]
    }
   ],
   "source": [
    "feature_prediction, feature_predict_pr, feature_allcoefs = log_reg(train_features,train_target,dev_features)\n",
    "ts_prediction, ts_predict_pr, ts_allcoefs = log_reg(train_ts,train_target,dev_ts)\n",
    "print feature_predict_pr\n",
    "print ts_predict_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_array=[]\n",
    "predict_score=[]\n",
    "score_val=0.1\n",
    "prediction=[]\n",
    "for i in range(len(dev_features)):\n",
    "    proba=[]\n",
    "    proba.append(msg_prob[i])\n",
    "    proba.append(title_prob[i])\n",
    "    proba.append(feature_predict_pr[i,1])\n",
    "    proba.append(ts_predict_pr[i,1])\n",
    "    prediction_array.append(proba)\n",
    "    predict_score.append(sum(proba))\n",
    "    if sum(proba)>=score_val:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of final model is 0.413\n"
     ]
    }
   ],
   "source": [
    "print \"F1 score of final model is \" + str(round(metrics.f1_score(dev_target,prediction),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
