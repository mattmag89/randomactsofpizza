{
 "metadata": {
  "name": "",
  "signature": "sha256:af4733296d859827a14e97692618e62f61a560339c3f8eee308cf7873c30c4a7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import re\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "from pprint import pprint\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dataset = 'train.json'\n",
      "test_dataset = 'test.json'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def separate_data(dataset):\n",
      "    data_target=[]\n",
      "    data_elements=[]\n",
      "    data_full=[]\n",
      "    data_msgtxt=[]\n",
      "\n",
      "    for element in dataset[0]:\n",
      "        if element != 'requester_received_pizza':\n",
      "            data_elements.append(element)\n",
      "\n",
      "    for request in dataset:\n",
      "        parts = []\n",
      "        data_msgtxt.append(request['request_text_edit_aware'])\n",
      "        if request['requester_received_pizza'] == True:\n",
      "            data_target.append(1)\n",
      "        else:\n",
      "            data_target.append(0)\n",
      "        for element in data_elements:\n",
      "            parts.append(request[element])\n",
      "        data_full.append(parts)\n",
      "\n",
      "    #pprint(msgtxt[0:3])\n",
      "    #pprint(data_elements)\n",
      "    #pprint(data_full[0])\n",
      "    return data_target, data_msgtxt, data_elements, data_full"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data=json.loads(open(train_dataset).read())\n",
      "num_train = len(raw_data)\n",
      "train_raw_data=raw_data[num_train/4:]\n",
      "dev_raw_data=raw_data[:num_train/4]\n",
      "train_target, train_msgtxt, train_elements, train_full = separate_data(train_raw_data)\n",
      "dev_target, dev_msgtxt, dev_elements, dev_full = separate_data(dev_raw_data)\n",
      "print num_train\n",
      "print len(train_target)\n",
      "print len(dev_target)\n",
      "pprint(train_full[0])\n",
      "pprint(train_target[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4040\n",
        "3030\n",
        "1010\n",
        "[0,\n",
        " [u'AdviceAnimals',\n",
        "  u'AskReddit',\n",
        "  u'Drugs',\n",
        "  u'Fitness',\n",
        "  u'IAmA',\n",
        "  u'ImGoingToHellForThis',\n",
        "  u'Juicing',\n",
        "  u'LearnUselessTalents',\n",
        "  u'LucidDreaming',\n",
        "  u'Music',\n",
        "  u'RandomActsOfGaming',\n",
        "  u'RandomActsOfPizza',\n",
        "  u'Random_Acts_Of_Pizza',\n",
        "  u'ShittyPoetry',\n",
        "  u'Steam',\n",
        "  u'WTF',\n",
        "  u'askscience',\n",
        "  u'atheism',\n",
        "  u'aww',\n",
        "  u'dubstep',\n",
        "  u'ecycle',\n",
        "  u'fffffffuuuuuuuuuuuu',\n",
        "  u'freebies',\n",
        "  u'funny',\n",
        "  u'gaming',\n",
        "  u'gifs',\n",
        "  u'mildlyinteresting',\n",
        "  u'movies',\n",
        "  u'opiates',\n",
        "  u'pics',\n",
        "  u'politics',\n",
        "  u'promos',\n",
        "  u'questions',\n",
        "  u'science',\n",
        "  u'slavery',\n",
        "  u'technology',\n",
        "  u'techsupport',\n",
        "  u'todayilearned',\n",
        "  u'trees',\n",
        "  u'vagina',\n",
        "  u'videos',\n",
        "  u'wikipedia',\n",
        "  u'worldnews'],\n",
        " 1,\n",
        " 341,\n",
        " u\"[Request] It's been a rough week, I need something to cheer me up.\",\n",
        " 279.53678240740743,\n",
        " u'N/A',\n",
        " 0.0,\n",
        " False,\n",
        " 431.98866898148145,\n",
        " 4037,\n",
        " 190,\n",
        " None,\n",
        " 3332,\n",
        " u'loganlulz',\n",
        " 1362165698.0,\n",
        " 9260,\n",
        " 1362165698.0,\n",
        " 3,\n",
        " 2,\n",
        " 1,\n",
        " 2,\n",
        " 168,\n",
        " 395,\n",
        " u\"Last Tuesday my friend of 6 years shot himself in the head and killed 3 other random people.\\n\\nIt's been a rough 2 weeks thinking about the whole thing. I haven't ate much and can barely get up in the morning to go to work.\\n\\n\",\n",
        " 711.5254513888889,\n",
        " 10369,\n",
        " u\"Last Tuesday my friend of 6 years shot himself in the head and killed 3 other random people.\\n\\nIt's been a rough 2 weeks thinking about the whole thing. I haven't ate much and can barely get up in the morning to go to work.\\n\\n\",\n",
        " u't3_19h8rg',\n",
        " 0,\n",
        " 43]\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize(train_data,dev_data):\n",
      "    # transform the train data\n",
      "    vectorizer_train = CountVectorizer()\n",
      "    #vectorizer_train = TfidfVectorizer()\n",
      "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
      "    vocab_train = vectorizer_train.get_feature_names()\n",
      "    # transform the dev data using the same vocab\n",
      "    vectorizer_dev = CountVectorizer(vocabulary=vocab_train)\n",
      "    #vectorizer_dev = TfidfVectorizer(vocabulary=vocab_train)\n",
      "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
      "    return v_data_train, v_data_dev, vocab_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_bigram(train_data,dev_data):\n",
      "    # transform the train data\n",
      "    vectorizer_train = CountVectorizer(ngram_range=(2,2))\n",
      "    v_data_train = vectorizer_train.fit_transform(train_data)\n",
      "    vocab_train = vectorizer_train.get_feature_names()\n",
      "    # transform the dev data using the same vocab\n",
      "    vectorizer_dev = CountVectorizer(ngram_range=(2,2),vocabulary=vocab_train)\n",
      "    v_data_dev = vectorizer_dev.fit_transform(dev_data)\n",
      "    return v_data_train, v_data_dev, vocab_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get attributes using nnz and shape\n",
      "def vectorizer_attrs(v_data):\n",
      "    nonzero = v_data.nnz\n",
      "    examples = v_data.shape[0]\n",
      "    distinct_words = v_data.shape[1]\n",
      "    avg_nonzero = float(nonzero)/examples\n",
      "    total_entries = examples*distinct_words\n",
      "    pct_nz_entries = float(nonzero)/total_entries*100\n",
      "    return \"Vocabulary size: \" + str(distinct_words) + \"\\nAverage non-zero features per example: \" + str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_reg(train_data,train_label,dev_data,dev_label):\n",
      "    lor = LogisticRegression(penalty='l2',C=100)\n",
      "    lor.fit(train_data, train_label)\n",
      "    lor_pred = lor.predict(dev_data)\n",
      "    lor_pred_pr = lor.predict_proba(dev_data)\n",
      "    allcoefs = lor.coef_.copy()\n",
      "    f1scr = metrics.f1_score(dev_label,lor_pred)\n",
      "    # Return the prediction matrix, coefficients, and f1 score\n",
      "    return lor_pred, lor_pred_pr, allcoefs, f1scr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_topn(top_n,lorcoefs,vocab):\n",
      "    allcoefs = lorcoefs.copy()\n",
      "    lbls=allcoefs.shape[0]\n",
      "    index=[]\n",
      "    words=[]\n",
      "    for num in range(top_n):\n",
      "        mxindex = allcoefs.argmax(axis=1)\n",
      "        for lbl in range(lbls):\n",
      "            allcoefs[lbl][mxindex[lbl]] = 0\n",
      "            index.append(mxindex[lbl])\n",
      "            words.append(vocab[mxindex[lbl]])\n",
      "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
      "    coefs=np.zeros((len(index),lbls))\n",
      "    for lbl in range(lbls):\n",
      "        for element in range(len(index)):\n",
      "            coefs[element][lbl] = lorcoefs[lbl][index[element]]\n",
      "    return words, coefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_vdata, dev_vdata, vocab = vectorize(train_msgtxt, dev_msgtxt)\n",
      "prediction, predict_pr, allcoefs, f1 = log_reg(train_vdata,train_target,dev_vdata,dev_target)\n",
      "words, coefs = get_topn(10,allcoefs, vocab)\n",
      "print vectorizer_attrs(train_vdata)\n",
      "print prediction\n",
      "print predict_pr[:,1]\n",
      "print f1\n",
      "print words\n",
      "print coefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Vocabulary size: 10556\n",
        "Average non-zero features per example: 53.4\n",
        "Fraction of non-zero entries in the matrix is 161815/31984680 (0.51%)\n",
        "[0 0 0 ..., 0 0 0]\n",
        "[  3.65580213e-01   2.29842605e-01   2.24985738e-04 ...,   4.08809113e-05\n",
        "   8.70104077e-02   4.59962003e-02]\n",
        "0.287401574803\n",
        "[u'married', u'lexically', u'ranch', u'hurting', u'yall', u'bloke', u'quick', u'father', u'biggie', u'handouts']\n",
        "[[ 5.63980676]\n",
        " [ 5.27709072]\n",
        " [ 4.85778616]\n",
        " [ 4.7115559 ]\n",
        " [ 4.61502359]\n",
        " [ 4.56727042]\n",
        " [ 4.49364985]\n",
        " [ 4.4468339 ]\n",
        " [ 4.35744592]\n",
        " [ 4.34939963]]\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}