{
 "metadata": {
  "name": "",
  "signature": "sha256:f5e8f2874b23b04658e340e1d22b6064e168157cb6d658bfde62969e9fc48943"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import re\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "from pprint import pprint\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dataset = 'train.json'\n",
      "test_dataset = 'test.json'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def separate_data(dataset):\n",
      "    data_target=[]\n",
      "    data_elements=[]\n",
      "    data_full=[]\n",
      "    data_msgtxt=[]\n",
      "\n",
      "    for element in dataset[0]:\n",
      "        if element != 'requester_received_pizza':\n",
      "            data_elements.append(element)\n",
      "\n",
      "    for request in dataset:\n",
      "        parts = []\n",
      "        data_msgtxt.append(request['request_text_edit_aware'])\n",
      "        if request['requester_received_pizza'] == True:\n",
      "            data_target.append(1)\n",
      "        else:\n",
      "            data_target.append(0)\n",
      "        for element in data_elements:\n",
      "            parts.append(request[element])\n",
      "        data_full.append(parts)\n",
      "\n",
      "    #pprint(msgtxt[0:3])\n",
      "    #pprint(data_elements)\n",
      "    #pprint(data_full[0])\n",
      "    return data_target, data_msgtxt, data_elements, data_full"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data=json.loads(open(dataset).read())\n",
      "num_test = len(raw_data)\n",
      "train_raw_data=raw_data[num_test/4:]\n",
      "dev_raw_data=raw_data[:num_test/4]\n",
      "train_target, train_msgtxt, train_elements, train_full = separate_data(train_raw_data)\n",
      "dev_target, dev_msgtxt, dev_elements, dev_full = separate_data(dev_raw_data)\n",
      "print num_test\n",
      "print len(train_target)\n",
      "print len(dev_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4040\n",
        "3030\n",
        "1010\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize(data):\n",
      "    # Initialize a vectorizer\n",
      "    vectorizer = CountVectorizer()\n",
      "    # Run fit_transform on our full training data set\n",
      "    v_data = vectorizer.fit_transform(data)\n",
      "    return v_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get attributes using nnz and shape\n",
      "def vectorizer_attrs(v_data):\n",
      "    nonzero = v_data.nnz\n",
      "    examples = v_data.shape[0]\n",
      "    distinct_words = v_data.shape[1]\n",
      "    avg_nonzero = float(nonzero)/examples\n",
      "    total_entries = examples*distinct_words\n",
      "    pct_nz_entries = float(nonzero)/total_entries*100\n",
      "    return \"Vocabulary size: \" + str(distinct_words) + \"\\nAverage non-zero features per example: \" + str(round(avg_nonzero,1)) + \"\\nFraction of non-zero entries in the matrix is \" + str(nonzero) + \"/\" + str(total_entries) + \" (\" + str(round(pct_nz_entries,2)) + \"%)\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_reg(train_data,train_label,dev_data,dev_label):\n",
      "    lor = LogisticRegression()\n",
      "    lor.fit(train_data, train_label)\n",
      "    lor_pred = lor.predict(dev_data)\n",
      "    # Return the prediction matrix and the f1 score\n",
      "    f1scr = metrics.f1_score(dev_label,lor_pred)\n",
      "    return lor_pred, f1scr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_vdata = vectorize(train_msgtxt)\n",
      "dev_vdata = vectorize(dev_msgtxt)\n",
      "prediction, f1 = log_reg(train_vdata,train_target,dev_vdata,dev_target)\n",
      "print f1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "X has 6043 features per sample; expecting 10556",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-99-db88d33d1e91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_vdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_msgtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdev_vdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_msgtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_vdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-98-b766d7eecde9>\u001b[0m in \u001b[0;36mlog_reg\u001b[1;34m(train_data, train_label, dev_data, dev_label)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlor_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Return the prediction matrix and the f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf1scr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlor_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 196\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
        "\u001b[1;31mValueError\u001b[0m: X has 6043 features per sample; expecting 10556"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First I wrote a function to return the coefficient matrix and list of top n words for \n",
      "def get_topn(top_n,data,labels,vocab):\n",
      "    # Train LR with default settings (not the optimized C from last problem!)\n",
      "    lor = LogisticRegression()\n",
      "    lor.fit(data, labels)\n",
      "    # Cycle over coefficients to get n highest for each label\n",
      "    # We'll create a copy of the coefficient matrix and set the maxes in the copy to 0 on each iteration\n",
      "    allcoefs = lor.coef_.copy()\n",
      "    lbls=allcoefs.shape[0]\n",
      "    index=[]\n",
      "    words=[]\n",
      "    for num in range(top_n):\n",
      "        mxindex = allcoefs.argmax(axis=1)\n",
      "        for lbl in range(lbls):\n",
      "            allcoefs[lbl][mxindex[lbl]] = 0\n",
      "            index.append(mxindex[lbl])\n",
      "            words.append(vocab[mxindex[lbl]])\n",
      "    # With our new index of the top n words in each label, get the coefficient matrix of these words\n",
      "    coefs=np.zeros((len(index),lbls))\n",
      "    for lbl in range(lbls):\n",
      "        for element in range(len(index)):\n",
      "            coefs[element][lbl] = lor.coef_[lbl][index[element]]\n",
      "    return words, coefs\n",
      "\n",
      "vectorizerA = CountVectorizer()\n",
      "train_v = vectorizerA.fit_transform(train_msgtxt)\n",
      "vocab_train = vectorizerA.get_feature_names()\n",
      "words,coefs=get_topn(25,train_v,train_target,vocab_train)\n",
      "print \"Top words among our labels:\"\n",
      "print words\n",
      "print \"Coefficient matrix for the top words:\"\n",
      "print coefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top words among our labels:\n",
        "[u'hurting', u'father', u'losing', u'expected', u'surprise', u'pockets', u'sunday', u'mentioned', u'married', u'battle', u'mention', u'deliver', u'except', u'aren', u'compensation', u'cheeses', u'ranch', u'nobody', u'stopped', u'biggie', u'fair', u'checks', u'handouts', u'constantly', u'exchange']\n",
        "Coefficient matrix for the top words:\n",
        "[[ 1.38979149]\n",
        " [ 1.17867252]\n",
        " [ 1.12105558]\n",
        " [ 1.11920295]\n",
        " [ 1.11575959]\n",
        " [ 1.11324099]\n",
        " [ 1.10625053]\n",
        " [ 1.09621664]\n",
        " [ 1.07325236]\n",
        " [ 1.03995868]\n",
        " [ 1.01594602]\n",
        " [ 1.01314347]\n",
        " [ 1.00906064]\n",
        " [ 1.00363151]\n",
        " [ 1.00332433]\n",
        " [ 0.99997657]\n",
        " [ 0.98447606]\n",
        " [ 0.97884885]\n",
        " [ 0.97201577]\n",
        " [ 0.96585918]\n",
        " [ 0.96008896]\n",
        " [ 0.95773977]\n",
        " [ 0.94259068]\n",
        " [ 0.94180933]\n",
        " [ 0.93609714]]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_bigram = CountVectorizer(ngram_range=(3,3))\n",
      "train_v = vectorizer_bigram.fit_transform(train_msgtxt)\n",
      "vocab_train = vectorizer_bigram.get_feature_names()\n",
      "words,coefs=get_topn(25,train_v,train_target,vocab_train)\n",
      "print \"Top words among our labels:\"\n",
      "print words\n",
      "print \"Coefficient matrix for the top words:\"\n",
      "print coefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top words among our labels:\n",
        "[u'op will deliver', u'to ask for', u'in north carolina', u'like to surprise', u'appreciate all of', u'blasting music and', u'got pizza thanks', u'music and hungry', u'pizza thanks lexically', u'http imgur com', u'some pizza love', u'would love pie', u'but would love', u'of would really', u'all of you', u'biochemistry is hell', u'hell of major', u'is hell of', u'for the pizza', u'that would be', u'forward when have', u'surprise my son', u'family of would', u'thanks so much', u'anything thanks so']\n",
        "Coefficient matrix for the top words:\n",
        "[[ 1.08699872]\n",
        " [ 0.83623287]\n",
        " [ 0.73977217]\n",
        " [ 0.70972807]\n",
        " [ 0.68181443]\n",
        " [ 0.66655323]\n",
        " [ 0.66655323]\n",
        " [ 0.66655323]\n",
        " [ 0.66655323]\n",
        " [ 0.66367049]\n",
        " [ 0.65824721]\n",
        " [ 0.64199377]\n",
        " [ 0.63261242]\n",
        " [ 0.61768298]\n",
        " [ 0.5817772 ]\n",
        " [ 0.5747451 ]\n",
        " [ 0.5747451 ]\n",
        " [ 0.5747451 ]\n",
        " [ 0.56770751]\n",
        " [ 0.54790156]\n",
        " [ 0.54251833]\n",
        " [ 0.52823284]\n",
        " [ 0.51691253]\n",
        " [ 0.51639638]\n",
        " [ 0.5051309 ]]\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}